{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "from scipy import stats\n",
    "import lightgbm as lgb\n",
    "#import data.jpx_tokyo_market_prediction\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.model_selection import cross_val_score, KFold, TimeSeriesSplit, GroupKFold, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "SEED=42\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_memory_usage(df):\n",
    "  \n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 197.98 MB\n",
      "Memory usage after optimization is: 102.33 MB\n",
      "Decreased by 48.3%\n",
      "Memory usage of dataframe is 22.91 MB\n",
      "Memory usage after optimization is: 11.84 MB\n",
      "Decreased by 48.3%\n"
     ]
    }
   ],
   "source": [
    "train = reduce_memory_usage(pd.read_csv(\"data/train_files/stock_prices.csv\"))\n",
    "train=train.drop(columns=['RowId','ExpectedDividend','AdjustmentFactor','SupervisionFlag']).dropna().reset_index(drop=True)\n",
    "val = reduce_memory_usage(pd.read_csv(\"data/supplemental_files/stock_prices.csv\"))\n",
    "val = val.drop(columns=['RowId','ExpectedDividend','AdjustmentFactor','SupervisionFlag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Date'] = pd.to_datetime(train['Date'])\n",
    "val['Date'] = pd.to_datetime(val['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(feats):\n",
    "    feats[\"return_1month\"] = feats[\"Close\"].pct_change(20)\n",
    "    feats[\"return_2month\"] = feats[\"Close\"].pct_change(40)\n",
    "    feats[\"return_3month\"] = feats[\"Close\"].pct_change(60)\n",
    "    feats[\"volatility_1month\"] = (\n",
    "        np.log(feats[\"Close\"]).diff().rolling(20).std()\n",
    "    )\n",
    "    feats[\"volatility_2month\"] = (\n",
    "        np.log(feats[\"Close\"]).diff().rolling(40).std()\n",
    "    )\n",
    "    feats[\"volatility_3month\"] = (\n",
    "        np.log(feats[\"Close\"]).diff().rolling(60).std()\n",
    "    )\n",
    "    feats[\"MA_gap_1month\"] = feats[\"Close\"] / (\n",
    "        feats[\"Close\"].rolling(20).mean()\n",
    "    )\n",
    "    feats[\"MA_gap_2month\"] = feats[\"Close\"] / (\n",
    "        feats[\"Close\"].rolling(40).mean()\n",
    "    )\n",
    "    feats[\"MA_gap_3month\"] = feats[\"Close\"] / (\n",
    "        feats[\"Close\"].rolling(60).mean()\n",
    "    )\n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = add_features(val)\n",
    "train = add_features(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(269881, 17)\n",
      "(2324923, 17)\n",
      "['2021-12-06T00:00:00.000000000' '2021-12-07T00:00:00.000000000'\n",
      " '2021-12-08T00:00:00.000000000' '2021-12-09T00:00:00.000000000'\n",
      " '2021-12-10T00:00:00.000000000' '2021-12-13T00:00:00.000000000'\n",
      " '2021-12-14T00:00:00.000000000' '2021-12-15T00:00:00.000000000'\n",
      " '2021-12-16T00:00:00.000000000' '2021-12-17T00:00:00.000000000'\n",
      " '2021-12-20T00:00:00.000000000' '2021-12-21T00:00:00.000000000'\n",
      " '2021-12-22T00:00:00.000000000' '2021-12-23T00:00:00.000000000'\n",
      " '2021-12-24T00:00:00.000000000' '2021-12-27T00:00:00.000000000'\n",
      " '2021-12-28T00:00:00.000000000' '2021-12-29T00:00:00.000000000'\n",
      " '2021-12-30T00:00:00.000000000' '2022-01-04T00:00:00.000000000'\n",
      " '2022-01-05T00:00:00.000000000' '2022-01-06T00:00:00.000000000'\n",
      " '2022-01-07T00:00:00.000000000' '2022-01-11T00:00:00.000000000'\n",
      " '2022-01-12T00:00:00.000000000' '2022-01-13T00:00:00.000000000'\n",
      " '2022-01-14T00:00:00.000000000' '2022-01-17T00:00:00.000000000'\n",
      " '2022-01-18T00:00:00.000000000' '2022-01-19T00:00:00.000000000'\n",
      " '2022-01-20T00:00:00.000000000' '2022-01-21T00:00:00.000000000'\n",
      " '2022-01-24T00:00:00.000000000' '2022-01-25T00:00:00.000000000'\n",
      " '2022-01-26T00:00:00.000000000' '2022-01-27T00:00:00.000000000'\n",
      " '2022-01-28T00:00:00.000000000' '2022-01-31T00:00:00.000000000'\n",
      " '2022-02-01T00:00:00.000000000' '2022-02-02T00:00:00.000000000'\n",
      " '2022-02-03T00:00:00.000000000' '2022-02-04T00:00:00.000000000'\n",
      " '2022-02-07T00:00:00.000000000' '2022-02-08T00:00:00.000000000'\n",
      " '2022-02-09T00:00:00.000000000' '2022-02-10T00:00:00.000000000'\n",
      " '2022-02-14T00:00:00.000000000' '2022-02-15T00:00:00.000000000'\n",
      " '2022-02-16T00:00:00.000000000' '2022-02-17T00:00:00.000000000'\n",
      " '2022-02-18T00:00:00.000000000' '2022-02-21T00:00:00.000000000'\n",
      " '2022-02-22T00:00:00.000000000' '2022-02-24T00:00:00.000000000'\n",
      " '2022-02-25T00:00:00.000000000' '2022-02-28T00:00:00.000000000'\n",
      " '2022-03-01T00:00:00.000000000' '2022-03-02T00:00:00.000000000'\n",
      " '2022-03-03T00:00:00.000000000' '2022-03-04T00:00:00.000000000'\n",
      " '2022-03-07T00:00:00.000000000' '2022-03-08T00:00:00.000000000'\n",
      " '2022-03-09T00:00:00.000000000' '2022-03-10T00:00:00.000000000'\n",
      " '2022-03-11T00:00:00.000000000' '2022-03-14T00:00:00.000000000'\n",
      " '2022-03-15T00:00:00.000000000' '2022-03-16T00:00:00.000000000'\n",
      " '2022-03-17T00:00:00.000000000' '2022-03-18T00:00:00.000000000'\n",
      " '2022-03-22T00:00:00.000000000' '2022-03-23T00:00:00.000000000'\n",
      " '2022-03-24T00:00:00.000000000' '2022-03-25T00:00:00.000000000'\n",
      " '2022-03-28T00:00:00.000000000' '2022-03-29T00:00:00.000000000'\n",
      " '2022-03-30T00:00:00.000000000' '2022-03-31T00:00:00.000000000'\n",
      " '2022-04-01T00:00:00.000000000' '2022-04-04T00:00:00.000000000'\n",
      " '2022-04-05T00:00:00.000000000' '2022-04-06T00:00:00.000000000'\n",
      " '2022-04-07T00:00:00.000000000' '2022-04-08T00:00:00.000000000'\n",
      " '2022-04-11T00:00:00.000000000' '2022-04-12T00:00:00.000000000'\n",
      " '2022-04-13T00:00:00.000000000' '2022-04-14T00:00:00.000000000'\n",
      " '2022-04-15T00:00:00.000000000' '2022-04-18T00:00:00.000000000'\n",
      " '2022-04-19T00:00:00.000000000' '2022-04-20T00:00:00.000000000'\n",
      " '2022-04-21T00:00:00.000000000' '2022-04-22T00:00:00.000000000'\n",
      " '2022-04-25T00:00:00.000000000' '2022-04-26T00:00:00.000000000'\n",
      " '2022-04-27T00:00:00.000000000' '2022-04-28T00:00:00.000000000'\n",
      " '2022-05-02T00:00:00.000000000' '2022-05-06T00:00:00.000000000'\n",
      " '2022-05-09T00:00:00.000000000' '2022-05-10T00:00:00.000000000'\n",
      " '2022-05-11T00:00:00.000000000' '2022-05-12T00:00:00.000000000'\n",
      " '2022-05-13T00:00:00.000000000' '2022-05-16T00:00:00.000000000'\n",
      " '2022-05-17T00:00:00.000000000' '2022-05-18T00:00:00.000000000'\n",
      " '2022-05-19T00:00:00.000000000' '2022-05-20T00:00:00.000000000'\n",
      " '2022-05-23T00:00:00.000000000' '2022-05-24T00:00:00.000000000'\n",
      " '2022-05-25T00:00:00.000000000' '2022-05-26T00:00:00.000000000'\n",
      " '2022-05-27T00:00:00.000000000' '2022-05-30T00:00:00.000000000'\n",
      " '2022-05-31T00:00:00.000000000' '2022-06-01T00:00:00.000000000'\n",
      " '2022-06-02T00:00:00.000000000' '2022-06-03T00:00:00.000000000'\n",
      " '2022-06-06T00:00:00.000000000' '2022-06-07T00:00:00.000000000'\n",
      " '2022-06-08T00:00:00.000000000' '2022-06-09T00:00:00.000000000'\n",
      " '2022-06-10T00:00:00.000000000' '2022-06-13T00:00:00.000000000'\n",
      " '2022-06-14T00:00:00.000000000' '2022-06-15T00:00:00.000000000'\n",
      " '2022-06-16T00:00:00.000000000' '2022-06-17T00:00:00.000000000'\n",
      " '2022-06-20T00:00:00.000000000' '2022-06-21T00:00:00.000000000'\n",
      " '2022-06-22T00:00:00.000000000' '2022-06-23T00:00:00.000000000'\n",
      " '2022-06-24T00:00:00.000000000']\n"
     ]
    }
   ],
   "source": [
    "print(val.shape)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep 2022-06-24 for the test set\n",
    "test = val[val['Date'] == '2022-06-24']\n",
    "val = val[val['Date'] != '2022-06-24']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feval_rmse(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'rmse', mean_squared_error(y_true, y_pred), False\n",
    "\n",
    "def feval_pearsonr(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'pearsonr', stats.pearsonr(y_true, y_pred)[0], True\n",
    "\n",
    "def calc_spread_return_per_day(df, portfolio_size=200, toprank_weight_ratio=2):\n",
    "    assert df['Rank'].min() == 0\n",
    "    assert df['Rank'].max() == len(df['Rank']) - 1\n",
    "    weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n",
    "    purchase = (df.sort_values(by='Rank')['Target'][:portfolio_size] * weights).sum() / weights.mean()\n",
    "    short = (df.sort_values(by='Rank', ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n",
    "    return purchase - short\n",
    "\n",
    "def calc_spread_return_sharpe(df: pd.DataFrame, portfolio_size=200, toprank_weight_ratio=2):\n",
    "    buf = df.groupby('Date').apply(calc_spread_return_per_day, portfolio_size, toprank_weight_ratio)\n",
    "    sharpe_ratio = buf.mean() / buf.std()\n",
    "    return sharpe_ratio#, buf\n",
    "\n",
    "def add_rank(df):\n",
    "    df[\"Rank\"] = df.groupby(\"Date\")[\"Target\"].rank(ascending=False, method=\"first\") - 1 \n",
    "    df[\"Rank\"] = df[\"Rank\"].astype(\"int\")\n",
    "    return df\n",
    "\n",
    "def fill_nan_inf(df):\n",
    "    df = df.fillna(0)\n",
    "    df = df.replace([np.inf, -np.inf], 0)\n",
    "    return df\n",
    "\n",
    "def check_score(df,preds,Securities_filter=[]):\n",
    "    tmp_preds=df[['Date','SecuritiesCode']].copy()\n",
    "    tmp_preds['Target']=preds\n",
    "    \n",
    "    #Rank Filter. Calculate median for this date and assign this value to the list of Securities to filter.\n",
    "    tmp_preds['target_mean']=tmp_preds.groupby(\"Date\")[\"Target\"].transform('median')\n",
    "    tmp_preds.loc[tmp_preds['SecuritiesCode'].isin(Securities_filter),'Target']=tmp_preds['target_mean']\n",
    "    \n",
    "    tmp_preds = add_rank(tmp_preds)\n",
    "    df['Rank']=tmp_preds['Rank']\n",
    "    score=round(calc_spread_return_sharpe(df, portfolio_size= 200, toprank_weight_ratio= 2),5)\n",
    "    score_mean=round(df.groupby('Date').apply(calc_spread_return_per_day, 200, 2).mean(),5)\n",
    "    score_std=round(df.groupby('Date').apply(calc_spread_return_per_day, 200, 2).std(),5)\n",
    "    print(f'Competition_Score:{score}, rank_score_mean:{score_mean}, rank_score_std:{score_std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train, val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_spred_l=list((all_data.groupby('SecuritiesCode')['Target'].max()-all_data.groupby('SecuritiesCode')['Target'].min()).sort_values()[:1000].index)\n",
    "list_spred_h=list((all_data.groupby('SecuritiesCode')['Target'].max()-all_data.groupby('SecuritiesCode')['Target'].min()).sort_values()[1000:].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's pearsonr: 0.0297045\tvalid_1's pearsonr: 0.0111404\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[2]\ttraining's pearsonr: 0.0297045\tvalid_1's pearsonr: 0.0111404\n",
      "[3]\ttraining's pearsonr: 0.0297175\tvalid_1's pearsonr: 0.0112094\n",
      "[4]\ttraining's pearsonr: 0.0297155\tvalid_1's pearsonr: 0.0111929\n",
      "[5]\ttraining's pearsonr: 0.0297186\tvalid_1's pearsonr: 0.0112224\n",
      "[6]\ttraining's pearsonr: 0.0320867\tvalid_1's pearsonr: 0.0121839\n",
      "[7]\ttraining's pearsonr: 0.0333168\tvalid_1's pearsonr: 0.0124904\n",
      "[8]\ttraining's pearsonr: 0.0329675\tvalid_1's pearsonr: 0.0123575\n",
      "[9]\ttraining's pearsonr: 0.0339463\tvalid_1's pearsonr: 0.0124117\n",
      "[10]\ttraining's pearsonr: 0.0346647\tvalid_1's pearsonr: 0.0124765\n",
      "[11]\ttraining's pearsonr: 0.0344319\tvalid_1's pearsonr: 0.0125227\n",
      "[12]\ttraining's pearsonr: 0.0342181\tvalid_1's pearsonr: 0.0125236\n",
      "[13]\ttraining's pearsonr: 0.0346622\tvalid_1's pearsonr: 0.0124359\n",
      "[14]\ttraining's pearsonr: 0.0349923\tvalid_1's pearsonr: 0.0124\n",
      "[15]\ttraining's pearsonr: 0.0349456\tvalid_1's pearsonr: 0.012417\n",
      "[16]\ttraining's pearsonr: 0.034849\tvalid_1's pearsonr: 0.0124006\n",
      "[17]\ttraining's pearsonr: 0.0350731\tvalid_1's pearsonr: 0.0122954\n",
      "[18]\ttraining's pearsonr: 0.0351908\tvalid_1's pearsonr: 0.0122265\n",
      "[19]\ttraining's pearsonr: 0.0357936\tvalid_1's pearsonr: 0.0124367\n",
      "[20]\ttraining's pearsonr: 0.0358906\tvalid_1's pearsonr: 0.0123472\n",
      "[21]\ttraining's pearsonr: 0.0363468\tvalid_1's pearsonr: 0.0125079\n",
      "[22]\ttraining's pearsonr: 0.0367565\tvalid_1's pearsonr: 0.0128192\n",
      "[23]\ttraining's pearsonr: 0.0367646\tvalid_1's pearsonr: 0.0127028\n",
      "[24]\ttraining's pearsonr: 0.0367225\tvalid_1's pearsonr: 0.0125974\n",
      "[25]\ttraining's pearsonr: 0.0370248\tvalid_1's pearsonr: 0.012796\n",
      "[26]\ttraining's pearsonr: 0.0370869\tvalid_1's pearsonr: 0.0127301\n",
      "[27]\ttraining's pearsonr: 0.037494\tvalid_1's pearsonr: 0.0128609\n",
      "[28]\ttraining's pearsonr: 0.037494\tvalid_1's pearsonr: 0.0127217\n",
      "[29]\ttraining's pearsonr: 0.0378186\tvalid_1's pearsonr: 0.0128256\n",
      "[30]\ttraining's pearsonr: 0.0380532\tvalid_1's pearsonr: 0.0127535\n",
      "[31]\ttraining's pearsonr: 0.0382355\tvalid_1's pearsonr: 0.0128737\n",
      "[32]\ttraining's pearsonr: 0.0384648\tvalid_1's pearsonr: 0.0129567\n",
      "[33]\ttraining's pearsonr: 0.0384899\tvalid_1's pearsonr: 0.0128356\n",
      "[34]\ttraining's pearsonr: 0.0387802\tvalid_1's pearsonr: 0.0129366\n",
      "[35]\ttraining's pearsonr: 0.0390108\tvalid_1's pearsonr: 0.0128918\n",
      "[36]\ttraining's pearsonr: 0.039167\tvalid_1's pearsonr: 0.0129557\n",
      "[37]\ttraining's pearsonr: 0.0393529\tvalid_1's pearsonr: 0.0131492\n",
      "[38]\ttraining's pearsonr: 0.03936\tvalid_1's pearsonr: 0.0130481\n",
      "[39]\ttraining's pearsonr: 0.0394724\tvalid_1's pearsonr: 0.0129943\n",
      "[40]\ttraining's pearsonr: 0.0395342\tvalid_1's pearsonr: 0.013133\n",
      "[41]\ttraining's pearsonr: 0.0396233\tvalid_1's pearsonr: 0.0130514\n",
      "[42]\ttraining's pearsonr: 0.039756\tvalid_1's pearsonr: 0.0130697\n",
      "[43]\ttraining's pearsonr: 0.040002\tvalid_1's pearsonr: 0.0130243\n",
      "[44]\ttraining's pearsonr: 0.0400943\tvalid_1's pearsonr: 0.013148\n",
      "[45]\ttraining's pearsonr: 0.040303\tvalid_1's pearsonr: 0.0130805\n",
      "[46]\ttraining's pearsonr: 0.0403735\tvalid_1's pearsonr: 0.0130379\n",
      "[47]\ttraining's pearsonr: 0.0405166\tvalid_1's pearsonr: 0.013095\n",
      "[48]\ttraining's pearsonr: 0.0406247\tvalid_1's pearsonr: 0.0130551\n",
      "[49]\ttraining's pearsonr: 0.0407327\tvalid_1's pearsonr: 0.012992\n",
      "[50]\ttraining's pearsonr: 0.0408882\tvalid_1's pearsonr: 0.0131462\n",
      "[51]\ttraining's pearsonr: 0.0409586\tvalid_1's pearsonr: 0.0131147\n",
      "[52]\ttraining's pearsonr: 0.041063\tvalid_1's pearsonr: 0.0130781\n",
      "[53]\ttraining's pearsonr: 0.041287\tvalid_1's pearsonr: 0.0130963\n",
      "[54]\ttraining's pearsonr: 0.0413458\tvalid_1's pearsonr: 0.0130331\n",
      "[55]\ttraining's pearsonr: 0.0413967\tvalid_1's pearsonr: 0.0131327\n",
      "[56]\ttraining's pearsonr: 0.0414993\tvalid_1's pearsonr: 0.0130897\n",
      "[57]\ttraining's pearsonr: 0.0415729\tvalid_1's pearsonr: 0.0131077\n",
      "[58]\ttraining's pearsonr: 0.0416599\tvalid_1's pearsonr: 0.0130605\n",
      "[59]\ttraining's pearsonr: 0.0417372\tvalid_1's pearsonr: 0.013151\n",
      "[60]\ttraining's pearsonr: 0.0418266\tvalid_1's pearsonr: 0.013106\n",
      "[61]\ttraining's pearsonr: 0.0419263\tvalid_1's pearsonr: 0.0130781\n",
      "[62]\ttraining's pearsonr: 0.0419954\tvalid_1's pearsonr: 0.013043\n",
      "[63]\ttraining's pearsonr: 0.0420784\tvalid_1's pearsonr: 0.0131677\n",
      "[64]\ttraining's pearsonr: 0.0421867\tvalid_1's pearsonr: 0.0131341\n",
      "[65]\ttraining's pearsonr: 0.0422369\tvalid_1's pearsonr: 0.0130798\n",
      "[66]\ttraining's pearsonr: 0.0424549\tvalid_1's pearsonr: 0.0131335\n",
      "[67]\ttraining's pearsonr: 0.0425076\tvalid_1's pearsonr: 0.0131155\n",
      "[68]\ttraining's pearsonr: 0.0425609\tvalid_1's pearsonr: 0.0130938\n",
      "[69]\ttraining's pearsonr: 0.0426876\tvalid_1's pearsonr: 0.0131283\n",
      "[70]\ttraining's pearsonr: 0.0428766\tvalid_1's pearsonr: 0.0131786\n",
      "[71]\ttraining's pearsonr: 0.0429547\tvalid_1's pearsonr: 0.0131587\n",
      "[72]\ttraining's pearsonr: 0.043128\tvalid_1's pearsonr: 0.0132048\n",
      "[73]\ttraining's pearsonr: 0.0432048\tvalid_1's pearsonr: 0.0131778\n",
      "[74]\ttraining's pearsonr: 0.0432687\tvalid_1's pearsonr: 0.0131659\n",
      "[75]\ttraining's pearsonr: 0.0434675\tvalid_1's pearsonr: 0.0132189\n",
      "[76]\ttraining's pearsonr: 0.0435148\tvalid_1's pearsonr: 0.013198\n",
      "[77]\ttraining's pearsonr: 0.0436583\tvalid_1's pearsonr: 0.0132396\n",
      "[78]\ttraining's pearsonr: 0.0436986\tvalid_1's pearsonr: 0.0132191\n",
      "[79]\ttraining's pearsonr: 0.0437375\tvalid_1's pearsonr: 0.0132032\n",
      "[80]\ttraining's pearsonr: 0.0438347\tvalid_1's pearsonr: 0.01323\n",
      "[81]\ttraining's pearsonr: 0.0439944\tvalid_1's pearsonr: 0.0132369\n",
      "[82]\ttraining's pearsonr: 0.0440463\tvalid_1's pearsonr: 0.0132129\n",
      "[83]\ttraining's pearsonr: 0.0442041\tvalid_1's pearsonr: 0.0132082\n",
      "[84]\ttraining's pearsonr: 0.0443253\tvalid_1's pearsonr: 0.0132478\n",
      "[85]\ttraining's pearsonr: 0.0444624\tvalid_1's pearsonr: 0.0132485\n",
      "[86]\ttraining's pearsonr: 0.0444802\tvalid_1's pearsonr: 0.0132204\n",
      "[87]\ttraining's pearsonr: 0.0446779\tvalid_1's pearsonr: 0.0132435\n",
      "[88]\ttraining's pearsonr: 0.0448049\tvalid_1's pearsonr: 0.0132366\n",
      "[89]\ttraining's pearsonr: 0.0448284\tvalid_1's pearsonr: 0.0132228\n",
      "[90]\ttraining's pearsonr: 0.0449792\tvalid_1's pearsonr: 0.0132195\n",
      "[91]\ttraining's pearsonr: 0.0451082\tvalid_1's pearsonr: 0.0132492\n",
      "[92]\ttraining's pearsonr: 0.0452455\tvalid_1's pearsonr: 0.0132438\n",
      "[93]\ttraining's pearsonr: 0.0452639\tvalid_1's pearsonr: 0.0132209\n",
      "[94]\ttraining's pearsonr: 0.0452844\tvalid_1's pearsonr: 0.0132055\n",
      "[95]\ttraining's pearsonr: 0.0453845\tvalid_1's pearsonr: 0.0132482\n",
      "[96]\ttraining's pearsonr: 0.0455075\tvalid_1's pearsonr: 0.0132421\n",
      "[97]\ttraining's pearsonr: 0.0456225\tvalid_1's pearsonr: 0.0132721\n",
      "[98]\ttraining's pearsonr: 0.0456604\tvalid_1's pearsonr: 0.01325\n",
      "[99]\ttraining's pearsonr: 0.0457503\tvalid_1's pearsonr: 0.0132405\n",
      "[100]\ttraining's pearsonr: 0.0457599\tvalid_1's pearsonr: 0.0132265\n",
      "[101]\ttraining's pearsonr: 0.0458434\tvalid_1's pearsonr: 0.0132657\n",
      "[102]\ttraining's pearsonr: 0.0459092\tvalid_1's pearsonr: 0.013253\n",
      "[103]\ttraining's pearsonr: 0.0459259\tvalid_1's pearsonr: 0.0132319\n",
      "[104]\ttraining's pearsonr: 0.0460847\tvalid_1's pearsonr: 0.0132486\n",
      "[105]\ttraining's pearsonr: 0.0461604\tvalid_1's pearsonr: 0.0132382\n",
      "[106]\ttraining's pearsonr: 0.0461713\tvalid_1's pearsonr: 0.013223\n",
      "[107]\ttraining's pearsonr: 0.0462762\tvalid_1's pearsonr: 0.0132818\n",
      "[108]\ttraining's pearsonr: 0.0464106\tvalid_1's pearsonr: 0.0132834\n",
      "[109]\ttraining's pearsonr: 0.046412\tvalid_1's pearsonr: 0.0132569\n",
      "[110]\ttraining's pearsonr: 0.0464832\tvalid_1's pearsonr: 0.0133162\n",
      "[111]\ttraining's pearsonr: 0.0465112\tvalid_1's pearsonr: 0.0132958\n",
      "[112]\ttraining's pearsonr: 0.0465958\tvalid_1's pearsonr: 0.0133123\n",
      "[113]\ttraining's pearsonr: 0.0468339\tvalid_1's pearsonr: 0.0133036\n",
      "[114]\ttraining's pearsonr: 0.0469023\tvalid_1's pearsonr: 0.0132998\n",
      "[115]\ttraining's pearsonr: 0.0469932\tvalid_1's pearsonr: 0.0133262\n",
      "[116]\ttraining's pearsonr: 0.0472061\tvalid_1's pearsonr: 0.0133158\n",
      "[117]\ttraining's pearsonr: 0.0473011\tvalid_1's pearsonr: 0.013299\n",
      "[118]\ttraining's pearsonr: 0.0475013\tvalid_1's pearsonr: 0.013303\n",
      "[119]\ttraining's pearsonr: 0.0476881\tvalid_1's pearsonr: 0.0132947\n",
      "[120]\ttraining's pearsonr: 0.0476934\tvalid_1's pearsonr: 0.0133047\n",
      "[121]\ttraining's pearsonr: 0.047876\tvalid_1's pearsonr: 0.0133077\n",
      "[122]\ttraining's pearsonr: 0.047952\tvalid_1's pearsonr: 0.0133049\n",
      "[123]\ttraining's pearsonr: 0.0481089\tvalid_1's pearsonr: 0.0133196\n",
      "[124]\ttraining's pearsonr: 0.0482674\tvalid_1's pearsonr: 0.0133294\n",
      "[125]\ttraining's pearsonr: 0.0484175\tvalid_1's pearsonr: 0.0133248\n",
      "[126]\ttraining's pearsonr: 0.0485057\tvalid_1's pearsonr: 0.0133169\n",
      "[127]\ttraining's pearsonr: 0.0486231\tvalid_1's pearsonr: 0.0133165\n",
      "[128]\ttraining's pearsonr: 0.0486704\tvalid_1's pearsonr: 0.0133226\n",
      "[129]\ttraining's pearsonr: 0.0487615\tvalid_1's pearsonr: 0.0133385\n",
      "[130]\ttraining's pearsonr: 0.0488732\tvalid_1's pearsonr: 0.0133414\n",
      "[131]\ttraining's pearsonr: 0.04894\tvalid_1's pearsonr: 0.0133997\n",
      "[132]\ttraining's pearsonr: 0.0490482\tvalid_1's pearsonr: 0.0134112\n",
      "[133]\ttraining's pearsonr: 0.0491201\tvalid_1's pearsonr: 0.0134064\n",
      "[134]\ttraining's pearsonr: 0.0492377\tvalid_1's pearsonr: 0.0134202\n",
      "[135]\ttraining's pearsonr: 0.0493248\tvalid_1's pearsonr: 0.0134271\n",
      "[136]\ttraining's pearsonr: 0.049326\tvalid_1's pearsonr: 0.0134449\n",
      "[137]\ttraining's pearsonr: 0.0494426\tvalid_1's pearsonr: 0.0134579\n",
      "[138]\ttraining's pearsonr: 0.0495244\tvalid_1's pearsonr: 0.0134752\n",
      "[139]\ttraining's pearsonr: 0.049597\tvalid_1's pearsonr: 0.0134666\n",
      "[140]\ttraining's pearsonr: 0.0497007\tvalid_1's pearsonr: 0.0134833\n",
      "[141]\ttraining's pearsonr: 0.0497974\tvalid_1's pearsonr: 0.013497\n",
      "[142]\ttraining's pearsonr: 0.0498835\tvalid_1's pearsonr: 0.0135119\n",
      "[143]\ttraining's pearsonr: 0.0499755\tvalid_1's pearsonr: 0.0134874\n",
      "[144]\ttraining's pearsonr: 0.0500459\tvalid_1's pearsonr: 0.013527\n",
      "[145]\ttraining's pearsonr: 0.0501371\tvalid_1's pearsonr: 0.013544\n",
      "[146]\ttraining's pearsonr: 0.0502226\tvalid_1's pearsonr: 0.0135417\n",
      "[147]\ttraining's pearsonr: 0.050258\tvalid_1's pearsonr: 0.013565\n",
      "[148]\ttraining's pearsonr: 0.0503147\tvalid_1's pearsonr: 0.0136078\n",
      "[149]\ttraining's pearsonr: 0.0503919\tvalid_1's pearsonr: 0.0136237\n",
      "[150]\ttraining's pearsonr: 0.0504655\tvalid_1's pearsonr: 0.0136153\n",
      "[151]\ttraining's pearsonr: 0.0505326\tvalid_1's pearsonr: 0.0136147\n",
      "[152]\ttraining's pearsonr: 0.0506563\tvalid_1's pearsonr: 0.0136653\n",
      "[153]\ttraining's pearsonr: 0.0507359\tvalid_1's pearsonr: 0.0136795\n",
      "[154]\ttraining's pearsonr: 0.0508462\tvalid_1's pearsonr: 0.0137352\n",
      "[155]\ttraining's pearsonr: 0.0509577\tvalid_1's pearsonr: 0.0137818\n",
      "[156]\ttraining's pearsonr: 0.0510351\tvalid_1's pearsonr: 0.0137997\n",
      "[157]\ttraining's pearsonr: 0.0511032\tvalid_1's pearsonr: 0.0138083\n",
      "[158]\ttraining's pearsonr: 0.0512445\tvalid_1's pearsonr: 0.0138459\n",
      "[159]\ttraining's pearsonr: 0.0513845\tvalid_1's pearsonr: 0.0138843\n",
      "[160]\ttraining's pearsonr: 0.0514474\tvalid_1's pearsonr: 0.0138934\n",
      "[161]\ttraining's pearsonr: 0.0515306\tvalid_1's pearsonr: 0.0138949\n",
      "[162]\ttraining's pearsonr: 0.0515802\tvalid_1's pearsonr: 0.0139044\n",
      "[163]\ttraining's pearsonr: 0.0517106\tvalid_1's pearsonr: 0.0139392\n",
      "[164]\ttraining's pearsonr: 0.0517638\tvalid_1's pearsonr: 0.0139482\n",
      "[165]\ttraining's pearsonr: 0.0519087\tvalid_1's pearsonr: 0.0139748\n",
      "[166]\ttraining's pearsonr: 0.0519826\tvalid_1's pearsonr: 0.0139815\n",
      "[167]\ttraining's pearsonr: 0.0520236\tvalid_1's pearsonr: 0.0139884\n",
      "[168]\ttraining's pearsonr: 0.0521607\tvalid_1's pearsonr: 0.0140121\n",
      "[169]\ttraining's pearsonr: 0.0522205\tvalid_1's pearsonr: 0.0140111\n",
      "[170]\ttraining's pearsonr: 0.0522793\tvalid_1's pearsonr: 0.0140271\n",
      "[171]\ttraining's pearsonr: 0.0523846\tvalid_1's pearsonr: 0.0140465\n",
      "[172]\ttraining's pearsonr: 0.0524406\tvalid_1's pearsonr: 0.0140623\n",
      "[173]\ttraining's pearsonr: 0.0525048\tvalid_1's pearsonr: 0.0140717\n",
      "[174]\ttraining's pearsonr: 0.0525543\tvalid_1's pearsonr: 0.0140834\n",
      "[175]\ttraining's pearsonr: 0.0526861\tvalid_1's pearsonr: 0.0141047\n",
      "[176]\ttraining's pearsonr: 0.0527996\tvalid_1's pearsonr: 0.0141264\n",
      "[177]\ttraining's pearsonr: 0.0528573\tvalid_1's pearsonr: 0.0141246\n",
      "[178]\ttraining's pearsonr: 0.0529134\tvalid_1's pearsonr: 0.0141602\n",
      "[179]\ttraining's pearsonr: 0.0529621\tvalid_1's pearsonr: 0.0141904\n",
      "[180]\ttraining's pearsonr: 0.0530823\tvalid_1's pearsonr: 0.0142068\n",
      "[181]\ttraining's pearsonr: 0.0531488\tvalid_1's pearsonr: 0.014218\n",
      "[182]\ttraining's pearsonr: 0.0531899\tvalid_1's pearsonr: 0.0142275\n",
      "[183]\ttraining's pearsonr: 0.0532773\tvalid_1's pearsonr: 0.0142362\n",
      "[184]\ttraining's pearsonr: 0.0533213\tvalid_1's pearsonr: 0.0142636\n",
      "[185]\ttraining's pearsonr: 0.0533823\tvalid_1's pearsonr: 0.0142742\n",
      "[186]\ttraining's pearsonr: 0.0534793\tvalid_1's pearsonr: 0.0142876\n",
      "[187]\ttraining's pearsonr: 0.0535194\tvalid_1's pearsonr: 0.0143001\n",
      "[188]\ttraining's pearsonr: 0.0536464\tvalid_1's pearsonr: 0.0143083\n",
      "[189]\ttraining's pearsonr: 0.0536983\tvalid_1's pearsonr: 0.0143049\n",
      "[190]\ttraining's pearsonr: 0.0537272\tvalid_1's pearsonr: 0.0143183\n",
      "[191]\ttraining's pearsonr: 0.0538359\tvalid_1's pearsonr: 0.0143243\n",
      "[192]\ttraining's pearsonr: 0.0539018\tvalid_1's pearsonr: 0.0143071\n",
      "[193]\ttraining's pearsonr: 0.0539268\tvalid_1's pearsonr: 0.0143089\n",
      "[194]\ttraining's pearsonr: 0.0539605\tvalid_1's pearsonr: 0.0143206\n",
      "[195]\ttraining's pearsonr: 0.0539867\tvalid_1's pearsonr: 0.0143326\n",
      "[196]\ttraining's pearsonr: 0.0540912\tvalid_1's pearsonr: 0.0143362\n",
      "[197]\ttraining's pearsonr: 0.0541584\tvalid_1's pearsonr: 0.0143454\n",
      "[198]\ttraining's pearsonr: 0.0541944\tvalid_1's pearsonr: 0.0143689\n",
      "[199]\ttraining's pearsonr: 0.0542445\tvalid_1's pearsonr: 0.0143693\n",
      "[200]\ttraining's pearsonr: 0.0543558\tvalid_1's pearsonr: 0.0143753\n",
      "[201]\ttraining's pearsonr: 0.054375\tvalid_1's pearsonr: 0.0143764\n",
      "[202]\ttraining's pearsonr: 0.0544104\tvalid_1's pearsonr: 0.0143933\n",
      "[203]\ttraining's pearsonr: 0.0544479\tvalid_1's pearsonr: 0.0144202\n",
      "[204]\ttraining's pearsonr: 0.0545247\tvalid_1's pearsonr: 0.0144191\n",
      "[205]\ttraining's pearsonr: 0.054619\tvalid_1's pearsonr: 0.0144183\n",
      "[206]\ttraining's pearsonr: 0.0546729\tvalid_1's pearsonr: 0.014419\n",
      "[207]\ttraining's pearsonr: 0.0547276\tvalid_1's pearsonr: 0.0144289\n",
      "[208]\ttraining's pearsonr: 0.054768\tvalid_1's pearsonr: 0.0144579\n",
      "[209]\ttraining's pearsonr: 0.054867\tvalid_1's pearsonr: 0.0144897\n",
      "[210]\ttraining's pearsonr: 0.0549089\tvalid_1's pearsonr: 0.0144913\n",
      "[211]\ttraining's pearsonr: 0.0549591\tvalid_1's pearsonr: 0.0145018\n",
      "[212]\ttraining's pearsonr: 0.0549869\tvalid_1's pearsonr: 0.0145152\n",
      "[213]\ttraining's pearsonr: 0.0550839\tvalid_1's pearsonr: 0.0145068\n",
      "[214]\ttraining's pearsonr: 0.0551439\tvalid_1's pearsonr: 0.0145006\n",
      "[215]\ttraining's pearsonr: 0.05517\tvalid_1's pearsonr: 0.0145019\n",
      "[216]\ttraining's pearsonr: 0.0552377\tvalid_1's pearsonr: 0.0145101\n",
      "[217]\ttraining's pearsonr: 0.0552729\tvalid_1's pearsonr: 0.0145109\n",
      "[218]\ttraining's pearsonr: 0.0553082\tvalid_1's pearsonr: 0.0145298\n",
      "[219]\ttraining's pearsonr: 0.0553452\tvalid_1's pearsonr: 0.0145507\n",
      "[220]\ttraining's pearsonr: 0.0553959\tvalid_1's pearsonr: 0.014553\n",
      "[221]\ttraining's pearsonr: 0.0554195\tvalid_1's pearsonr: 0.0145731\n",
      "[222]\ttraining's pearsonr: 0.0555354\tvalid_1's pearsonr: 0.014558\n",
      "[223]\ttraining's pearsonr: 0.0556375\tvalid_1's pearsonr: 0.0145613\n",
      "[224]\ttraining's pearsonr: 0.0557529\tvalid_1's pearsonr: 0.0145665\n",
      "[225]\ttraining's pearsonr: 0.0558576\tvalid_1's pearsonr: 0.0145671\n",
      "[226]\ttraining's pearsonr: 0.0559466\tvalid_1's pearsonr: 0.0145551\n",
      "[227]\ttraining's pearsonr: 0.0559688\tvalid_1's pearsonr: 0.0145495\n",
      "[228]\ttraining's pearsonr: 0.0559898\tvalid_1's pearsonr: 0.0145634\n",
      "[229]\ttraining's pearsonr: 0.0560998\tvalid_1's pearsonr: 0.0145667\n",
      "[230]\ttraining's pearsonr: 0.0562176\tvalid_1's pearsonr: 0.014547\n",
      "[231]\ttraining's pearsonr: 0.0562312\tvalid_1's pearsonr: 0.0145411\n",
      "[232]\ttraining's pearsonr: 0.0562889\tvalid_1's pearsonr: 0.0145349\n",
      "[233]\ttraining's pearsonr: 0.056382\tvalid_1's pearsonr: 0.014522\n",
      "[234]\ttraining's pearsonr: 0.0564044\tvalid_1's pearsonr: 0.0145217\n",
      "[235]\ttraining's pearsonr: 0.0565149\tvalid_1's pearsonr: 0.0145418\n",
      "[236]\ttraining's pearsonr: 0.0565851\tvalid_1's pearsonr: 0.0145456\n",
      "[237]\ttraining's pearsonr: 0.056691\tvalid_1's pearsonr: 0.0145417\n",
      "[238]\ttraining's pearsonr: 0.0567102\tvalid_1's pearsonr: 0.0145411\n",
      "[239]\ttraining's pearsonr: 0.0568156\tvalid_1's pearsonr: 0.01456\n",
      "[240]\ttraining's pearsonr: 0.056834\tvalid_1's pearsonr: 0.0145543\n",
      "[241]\ttraining's pearsonr: 0.0569315\tvalid_1's pearsonr: 0.0145529\n",
      "[242]\ttraining's pearsonr: 0.0570142\tvalid_1's pearsonr: 0.0145294\n",
      "[243]\ttraining's pearsonr: 0.057029\tvalid_1's pearsonr: 0.0145282\n",
      "[244]\ttraining's pearsonr: 0.0571124\tvalid_1's pearsonr: 0.0145173\n",
      "[245]\ttraining's pearsonr: 0.0571247\tvalid_1's pearsonr: 0.014516\n",
      "[246]\ttraining's pearsonr: 0.057214\tvalid_1's pearsonr: 0.0145148\n",
      "[247]\ttraining's pearsonr: 0.0573121\tvalid_1's pearsonr: 0.014532\n",
      "[248]\ttraining's pearsonr: 0.057364\tvalid_1's pearsonr: 0.0145827\n",
      "[249]\ttraining's pearsonr: 0.0574471\tvalid_1's pearsonr: 0.0145648\n",
      "[250]\ttraining's pearsonr: 0.0575054\tvalid_1's pearsonr: 0.0145501\n",
      "[251]\ttraining's pearsonr: 0.0575885\tvalid_1's pearsonr: 0.0145229\n",
      "[252]\ttraining's pearsonr: 0.0576363\tvalid_1's pearsonr: 0.0145716\n",
      "[253]\ttraining's pearsonr: 0.0577315\tvalid_1's pearsonr: 0.0145706\n",
      "[254]\ttraining's pearsonr: 0.0578208\tvalid_1's pearsonr: 0.0145627\n",
      "[255]\ttraining's pearsonr: 0.057843\tvalid_1's pearsonr: 0.014563\n",
      "[256]\ttraining's pearsonr: 0.0579197\tvalid_1's pearsonr: 0.0145689\n",
      "[257]\ttraining's pearsonr: 0.0580046\tvalid_1's pearsonr: 0.0145606\n",
      "[258]\ttraining's pearsonr: 0.0580972\tvalid_1's pearsonr: 0.0145334\n",
      "[259]\ttraining's pearsonr: 0.05817\tvalid_1's pearsonr: 0.0145391\n",
      "[260]\ttraining's pearsonr: 0.0582246\tvalid_1's pearsonr: 0.0145398\n",
      "[261]\ttraining's pearsonr: 0.0582943\tvalid_1's pearsonr: 0.0145488\n",
      "[262]\ttraining's pearsonr: 0.0583548\tvalid_1's pearsonr: 0.0145292\n",
      "[263]\ttraining's pearsonr: 0.058423\tvalid_1's pearsonr: 0.0145336\n",
      "[264]\ttraining's pearsonr: 0.0584704\tvalid_1's pearsonr: 0.0145494\n",
      "[265]\ttraining's pearsonr: 0.0585334\tvalid_1's pearsonr: 0.0145572\n",
      "[266]\ttraining's pearsonr: 0.0586113\tvalid_1's pearsonr: 0.0145366\n",
      "[267]\ttraining's pearsonr: 0.0586733\tvalid_1's pearsonr: 0.0145403\n",
      "[268]\ttraining's pearsonr: 0.0587309\tvalid_1's pearsonr: 0.0145478\n",
      "[269]\ttraining's pearsonr: 0.0588\tvalid_1's pearsonr: 0.0145602\n",
      "[270]\ttraining's pearsonr: 0.0588397\tvalid_1's pearsonr: 0.0145583\n",
      "[271]\ttraining's pearsonr: 0.0588948\tvalid_1's pearsonr: 0.0145659\n",
      "[272]\ttraining's pearsonr: 0.0589506\tvalid_1's pearsonr: 0.0145703\n",
      "[273]\ttraining's pearsonr: 0.0590314\tvalid_1's pearsonr: 0.0145587\n",
      "[274]\ttraining's pearsonr: 0.059094\tvalid_1's pearsonr: 0.0145593\n",
      "[275]\ttraining's pearsonr: 0.0591464\tvalid_1's pearsonr: 0.0145602\n",
      "[276]\ttraining's pearsonr: 0.0592131\tvalid_1's pearsonr: 0.0145763\n",
      "[277]\ttraining's pearsonr: 0.059274\tvalid_1's pearsonr: 0.0145747\n",
      "[278]\ttraining's pearsonr: 0.0593201\tvalid_1's pearsonr: 0.0145811\n",
      "[279]\ttraining's pearsonr: 0.0594013\tvalid_1's pearsonr: 0.01456\n",
      "[280]\ttraining's pearsonr: 0.0594553\tvalid_1's pearsonr: 0.014564\n",
      "[281]\ttraining's pearsonr: 0.0595081\tvalid_1's pearsonr: 0.0145648\n",
      "[282]\ttraining's pearsonr: 0.0595965\tvalid_1's pearsonr: 0.0145781\n",
      "[283]\ttraining's pearsonr: 0.0596693\tvalid_1's pearsonr: 0.0145935\n",
      "[284]\ttraining's pearsonr: 0.0597112\tvalid_1's pearsonr: 0.0145891\n",
      "[285]\ttraining's pearsonr: 0.0597724\tvalid_1's pearsonr: 0.0146041\n",
      "[286]\ttraining's pearsonr: 0.0598046\tvalid_1's pearsonr: 0.0145985\n",
      "[287]\ttraining's pearsonr: 0.0598405\tvalid_1's pearsonr: 0.014597\n",
      "[288]\ttraining's pearsonr: 0.0599204\tvalid_1's pearsonr: 0.0146087\n",
      "[289]\ttraining's pearsonr: 0.0599576\tvalid_1's pearsonr: 0.0146084\n",
      "[290]\ttraining's pearsonr: 0.0600305\tvalid_1's pearsonr: 0.0145967\n",
      "[291]\ttraining's pearsonr: 0.0600727\tvalid_1's pearsonr: 0.0145908\n",
      "[292]\ttraining's pearsonr: 0.0601425\tvalid_1's pearsonr: 0.0146019\n",
      "[293]\ttraining's pearsonr: 0.0602127\tvalid_1's pearsonr: 0.0146056\n",
      "[294]\ttraining's pearsonr: 0.060252\tvalid_1's pearsonr: 0.0146005\n",
      "[295]\ttraining's pearsonr: 0.0603173\tvalid_1's pearsonr: 0.0145833\n",
      "[296]\ttraining's pearsonr: 0.0603461\tvalid_1's pearsonr: 0.0145837\n",
      "[297]\ttraining's pearsonr: 0.0604037\tvalid_1's pearsonr: 0.0145884\n",
      "[298]\ttraining's pearsonr: 0.0604643\tvalid_1's pearsonr: 0.0145969\n",
      "[299]\ttraining's pearsonr: 0.0605043\tvalid_1's pearsonr: 0.0145947\n",
      "[300]\ttraining's pearsonr: 0.0605334\tvalid_1's pearsonr: 0.0145882\n",
      "[301]\ttraining's pearsonr: 0.0605882\tvalid_1's pearsonr: 0.0145994\n",
      "[302]\ttraining's pearsonr: 0.060621\tvalid_1's pearsonr: 0.0145974\n",
      "[303]\ttraining's pearsonr: 0.0606548\tvalid_1's pearsonr: 0.0145974\n",
      "[304]\ttraining's pearsonr: 0.0607011\tvalid_1's pearsonr: 0.0145916\n",
      "[305]\ttraining's pearsonr: 0.0607729\tvalid_1's pearsonr: 0.0145689\n",
      "[306]\ttraining's pearsonr: 0.0607973\tvalid_1's pearsonr: 0.0145691\n",
      "[307]\ttraining's pearsonr: 0.0608714\tvalid_1's pearsonr: 0.0145808\n",
      "[308]\ttraining's pearsonr: 0.0609424\tvalid_1's pearsonr: 0.0145872\n",
      "[309]\ttraining's pearsonr: 0.0609705\tvalid_1's pearsonr: 0.0145804\n",
      "[310]\ttraining's pearsonr: 0.0610252\tvalid_1's pearsonr: 0.0145818\n",
      "[311]\ttraining's pearsonr: 0.0610499\tvalid_1's pearsonr: 0.0145764\n",
      "[312]\ttraining's pearsonr: 0.061073\tvalid_1's pearsonr: 0.0145862\n",
      "[313]\ttraining's pearsonr: 0.0611405\tvalid_1's pearsonr: 0.0145623\n",
      "[314]\ttraining's pearsonr: 0.0612126\tvalid_1's pearsonr: 0.0145683\n",
      "[315]\ttraining's pearsonr: 0.0612832\tvalid_1's pearsonr: 0.0145759\n",
      "[316]\ttraining's pearsonr: 0.0613045\tvalid_1's pearsonr: 0.0145726\n",
      "[317]\ttraining's pearsonr: 0.0613332\tvalid_1's pearsonr: 0.0145673\n",
      "[318]\ttraining's pearsonr: 0.0613807\tvalid_1's pearsonr: 0.0145899\n",
      "[319]\ttraining's pearsonr: 0.0614303\tvalid_1's pearsonr: 0.0146008\n",
      "[320]\ttraining's pearsonr: 0.0614701\tvalid_1's pearsonr: 0.0145954\n",
      "[321]\ttraining's pearsonr: 0.0614871\tvalid_1's pearsonr: 0.0145949\n",
      "[322]\ttraining's pearsonr: 0.0615216\tvalid_1's pearsonr: 0.0145884\n",
      "[323]\ttraining's pearsonr: 0.0615907\tvalid_1's pearsonr: 0.01457\n",
      "[324]\ttraining's pearsonr: 0.0616238\tvalid_1's pearsonr: 0.01456\n",
      "[325]\ttraining's pearsonr: 0.0616541\tvalid_1's pearsonr: 0.0145617\n",
      "[326]\ttraining's pearsonr: 0.0617208\tvalid_1's pearsonr: 0.0145713\n",
      "[327]\ttraining's pearsonr: 0.0617791\tvalid_1's pearsonr: 0.0145776\n",
      "[328]\ttraining's pearsonr: 0.0618058\tvalid_1's pearsonr: 0.0145687\n",
      "[329]\ttraining's pearsonr: 0.0618539\tvalid_1's pearsonr: 0.0145763\n",
      "[330]\ttraining's pearsonr: 0.0619037\tvalid_1's pearsonr: 0.014581\n",
      "[331]\ttraining's pearsonr: 0.0619734\tvalid_1's pearsonr: 0.0145933\n",
      "[332]\ttraining's pearsonr: 0.0620479\tvalid_1's pearsonr: 0.0146038\n",
      "[333]\ttraining's pearsonr: 0.0620852\tvalid_1's pearsonr: 0.0145908\n",
      "[334]\ttraining's pearsonr: 0.0621492\tvalid_1's pearsonr: 0.0145985\n",
      "[335]\ttraining's pearsonr: 0.0622087\tvalid_1's pearsonr: 0.0145821\n",
      "[336]\ttraining's pearsonr: 0.0622371\tvalid_1's pearsonr: 0.0145736\n",
      "[337]\ttraining's pearsonr: 0.0623109\tvalid_1's pearsonr: 0.0145858\n",
      "[338]\ttraining's pearsonr: 0.0623759\tvalid_1's pearsonr: 0.0145972\n",
      "[339]\ttraining's pearsonr: 0.0623905\tvalid_1's pearsonr: 0.014597\n",
      "[340]\ttraining's pearsonr: 0.0624558\tvalid_1's pearsonr: 0.0146111\n",
      "[341]\ttraining's pearsonr: 0.0625198\tvalid_1's pearsonr: 0.0146101\n",
      "[342]\ttraining's pearsonr: 0.0625738\tvalid_1's pearsonr: 0.0146157\n",
      "[343]\ttraining's pearsonr: 0.0626355\tvalid_1's pearsonr: 0.0146219\n",
      "[344]\ttraining's pearsonr: 0.0627013\tvalid_1's pearsonr: 0.0146227\n",
      "[345]\ttraining's pearsonr: 0.0627304\tvalid_1's pearsonr: 0.0146111\n",
      "[346]\ttraining's pearsonr: 0.0627775\tvalid_1's pearsonr: 0.0146179\n",
      "[347]\ttraining's pearsonr: 0.0628457\tvalid_1's pearsonr: 0.0146157\n",
      "[348]\ttraining's pearsonr: 0.0628881\tvalid_1's pearsonr: 0.0146141\n",
      "[349]\ttraining's pearsonr: 0.0629168\tvalid_1's pearsonr: 0.0146071\n",
      "[350]\ttraining's pearsonr: 0.0629844\tvalid_1's pearsonr: 0.0146018\n",
      "[351]\ttraining's pearsonr: 0.0630305\tvalid_1's pearsonr: 0.0146049\n",
      "[352]\ttraining's pearsonr: 0.0630865\tvalid_1's pearsonr: 0.0146086\n",
      "[353]\ttraining's pearsonr: 0.0631566\tvalid_1's pearsonr: 0.0146205\n",
      "[354]\ttraining's pearsonr: 0.0632097\tvalid_1's pearsonr: 0.0146202\n",
      "[355]\ttraining's pearsonr: 0.0632232\tvalid_1's pearsonr: 0.0146167\n",
      "[356]\ttraining's pearsonr: 0.0632816\tvalid_1's pearsonr: 0.0146248\n",
      "[357]\ttraining's pearsonr: 0.0633449\tvalid_1's pearsonr: 0.0146197\n",
      "[358]\ttraining's pearsonr: 0.0633863\tvalid_1's pearsonr: 0.0146208\n",
      "[359]\ttraining's pearsonr: 0.0634475\tvalid_1's pearsonr: 0.0145989\n",
      "[360]\ttraining's pearsonr: 0.0635051\tvalid_1's pearsonr: 0.0145968\n",
      "[361]\ttraining's pearsonr: 0.0635677\tvalid_1's pearsonr: 0.0146102\n",
      "[362]\ttraining's pearsonr: 0.0636084\tvalid_1's pearsonr: 0.0146055\n",
      "[363]\ttraining's pearsonr: 0.0636657\tvalid_1's pearsonr: 0.0146085\n",
      "[364]\ttraining's pearsonr: 0.0637429\tvalid_1's pearsonr: 0.0146142\n",
      "[365]\ttraining's pearsonr: 0.0638077\tvalid_1's pearsonr: 0.0146385\n",
      "[366]\ttraining's pearsonr: 0.0638779\tvalid_1's pearsonr: 0.0146513\n",
      "[367]\ttraining's pearsonr: 0.0639308\tvalid_1's pearsonr: 0.0146499\n",
      "[368]\ttraining's pearsonr: 0.064004\tvalid_1's pearsonr: 0.014657\n",
      "[369]\ttraining's pearsonr: 0.0640488\tvalid_1's pearsonr: 0.014642\n",
      "[370]\ttraining's pearsonr: 0.0640876\tvalid_1's pearsonr: 0.0146438\n",
      "[371]\ttraining's pearsonr: 0.0641431\tvalid_1's pearsonr: 0.0146439\n",
      "[372]\ttraining's pearsonr: 0.0641679\tvalid_1's pearsonr: 0.0146352\n",
      "[373]\ttraining's pearsonr: 0.0642109\tvalid_1's pearsonr: 0.0146387\n",
      "[374]\ttraining's pearsonr: 0.0642622\tvalid_1's pearsonr: 0.0146368\n",
      "[375]\ttraining's pearsonr: 0.0643048\tvalid_1's pearsonr: 0.0146392\n",
      "[376]\ttraining's pearsonr: 0.064369\tvalid_1's pearsonr: 0.0146458\n",
      "[377]\ttraining's pearsonr: 0.0645089\tvalid_1's pearsonr: 0.0146557\n",
      "[378]\ttraining's pearsonr: 0.064556\tvalid_1's pearsonr: 0.0146588\n",
      "[379]\ttraining's pearsonr: 0.0645893\tvalid_1's pearsonr: 0.014673\n",
      "[380]\ttraining's pearsonr: 0.0647253\tvalid_1's pearsonr: 0.0146823\n",
      "[381]\ttraining's pearsonr: 0.0647748\tvalid_1's pearsonr: 0.0146802\n",
      "[382]\ttraining's pearsonr: 0.0648321\tvalid_1's pearsonr: 0.0146614\n",
      "[383]\ttraining's pearsonr: 0.0648923\tvalid_1's pearsonr: 0.0146844\n",
      "[384]\ttraining's pearsonr: 0.0649534\tvalid_1's pearsonr: 0.0146917\n",
      "[385]\ttraining's pearsonr: 0.0650251\tvalid_1's pearsonr: 0.0146983\n",
      "[386]\ttraining's pearsonr: 0.065157\tvalid_1's pearsonr: 0.014707\n",
      "[387]\ttraining's pearsonr: 0.0651971\tvalid_1's pearsonr: 0.0147036\n",
      "[388]\ttraining's pearsonr: 0.0652486\tvalid_1's pearsonr: 0.0147079\n",
      "[389]\ttraining's pearsonr: 0.0652877\tvalid_1's pearsonr: 0.0147112\n",
      "[390]\ttraining's pearsonr: 0.0653269\tvalid_1's pearsonr: 0.0147271\n",
      "[391]\ttraining's pearsonr: 0.0653835\tvalid_1's pearsonr: 0.014734\n",
      "[392]\ttraining's pearsonr: 0.0654408\tvalid_1's pearsonr: 0.0147463\n",
      "[393]\ttraining's pearsonr: 0.0654847\tvalid_1's pearsonr: 0.0147447\n",
      "[394]\ttraining's pearsonr: 0.065527\tvalid_1's pearsonr: 0.0147676\n",
      "[395]\ttraining's pearsonr: 0.0655787\tvalid_1's pearsonr: 0.0147733\n",
      "[396]\ttraining's pearsonr: 0.0657044\tvalid_1's pearsonr: 0.0147798\n",
      "[397]\ttraining's pearsonr: 0.0657586\tvalid_1's pearsonr: 0.0147808\n",
      "[398]\ttraining's pearsonr: 0.0658059\tvalid_1's pearsonr: 0.0147854\n",
      "[399]\ttraining's pearsonr: 0.0658551\tvalid_1's pearsonr: 0.0147703\n",
      "[400]\ttraining's pearsonr: 0.0659005\tvalid_1's pearsonr: 0.0147698\n",
      "[401]\ttraining's pearsonr: 0.06594\tvalid_1's pearsonr: 0.0147676\n",
      "[402]\ttraining's pearsonr: 0.0659766\tvalid_1's pearsonr: 0.0147668\n",
      "[403]\ttraining's pearsonr: 0.0660216\tvalid_1's pearsonr: 0.0147683\n",
      "[404]\ttraining's pearsonr: 0.0660808\tvalid_1's pearsonr: 0.0147748\n",
      "[405]\ttraining's pearsonr: 0.0661167\tvalid_1's pearsonr: 0.0147896\n",
      "[406]\ttraining's pearsonr: 0.0661598\tvalid_1's pearsonr: 0.0147938\n",
      "[407]\ttraining's pearsonr: 0.0662815\tvalid_1's pearsonr: 0.0147959\n",
      "[408]\ttraining's pearsonr: 0.0663166\tvalid_1's pearsonr: 0.0148117\n",
      "[409]\ttraining's pearsonr: 0.0663796\tvalid_1's pearsonr: 0.0148212\n",
      "[410]\ttraining's pearsonr: 0.0664317\tvalid_1's pearsonr: 0.0147992\n",
      "[411]\ttraining's pearsonr: 0.0664713\tvalid_1's pearsonr: 0.0148031\n",
      "[412]\ttraining's pearsonr: 0.0665387\tvalid_1's pearsonr: 0.0148082\n",
      "[413]\ttraining's pearsonr: 0.0666113\tvalid_1's pearsonr: 0.0148141\n",
      "[414]\ttraining's pearsonr: 0.0666443\tvalid_1's pearsonr: 0.014828\n",
      "[415]\ttraining's pearsonr: 0.0666942\tvalid_1's pearsonr: 0.0148293\n",
      "[416]\ttraining's pearsonr: 0.066752\tvalid_1's pearsonr: 0.0148441\n",
      "[417]\ttraining's pearsonr: 0.0667984\tvalid_1's pearsonr: 0.0148363\n",
      "[418]\ttraining's pearsonr: 0.0668523\tvalid_1's pearsonr: 0.0148442\n",
      "[419]\ttraining's pearsonr: 0.0669709\tvalid_1's pearsonr: 0.014851\n",
      "[420]\ttraining's pearsonr: 0.0670201\tvalid_1's pearsonr: 0.0148594\n",
      "[421]\ttraining's pearsonr: 0.0670521\tvalid_1's pearsonr: 0.0148742\n",
      "[422]\ttraining's pearsonr: 0.067088\tvalid_1's pearsonr: 0.0148808\n",
      "[423]\ttraining's pearsonr: 0.0671148\tvalid_1's pearsonr: 0.0148836\n",
      "[424]\ttraining's pearsonr: 0.0671667\tvalid_1's pearsonr: 0.0148879\n",
      "[425]\ttraining's pearsonr: 0.0672316\tvalid_1's pearsonr: 0.014892\n",
      "[426]\ttraining's pearsonr: 0.0672797\tvalid_1's pearsonr: 0.0148833\n",
      "[427]\ttraining's pearsonr: 0.067318\tvalid_1's pearsonr: 0.0148786\n",
      "[428]\ttraining's pearsonr: 0.0673741\tvalid_1's pearsonr: 0.0148827\n",
      "[429]\ttraining's pearsonr: 0.0674876\tvalid_1's pearsonr: 0.0148839\n",
      "[430]\ttraining's pearsonr: 0.0675347\tvalid_1's pearsonr: 0.014882\n",
      "[431]\ttraining's pearsonr: 0.0675772\tvalid_1's pearsonr: 0.0148882\n",
      "[432]\ttraining's pearsonr: 0.0676307\tvalid_1's pearsonr: 0.0148785\n",
      "[433]\ttraining's pearsonr: 0.0676767\tvalid_1's pearsonr: 0.014883\n",
      "[434]\ttraining's pearsonr: 0.0677297\tvalid_1's pearsonr: 0.0148758\n",
      "[435]\ttraining's pearsonr: 0.0677797\tvalid_1's pearsonr: 0.0148812\n",
      "[436]\ttraining's pearsonr: 0.0678483\tvalid_1's pearsonr: 0.0148723\n",
      "[437]\ttraining's pearsonr: 0.067906\tvalid_1's pearsonr: 0.0148831\n",
      "[438]\ttraining's pearsonr: 0.068017\tvalid_1's pearsonr: 0.0148905\n",
      "[439]\ttraining's pearsonr: 0.068048\tvalid_1's pearsonr: 0.0149032\n",
      "[440]\ttraining's pearsonr: 0.0680852\tvalid_1's pearsonr: 0.0149119\n",
      "[441]\ttraining's pearsonr: 0.0681349\tvalid_1's pearsonr: 0.0149077\n",
      "[442]\ttraining's pearsonr: 0.0681644\tvalid_1's pearsonr: 0.0149102\n",
      "[443]\ttraining's pearsonr: 0.0681927\tvalid_1's pearsonr: 0.0149226\n",
      "[444]\ttraining's pearsonr: 0.0682564\tvalid_1's pearsonr: 0.0149249\n",
      "[445]\ttraining's pearsonr: 0.068326\tvalid_1's pearsonr: 0.0149173\n",
      "[446]\ttraining's pearsonr: 0.0683783\tvalid_1's pearsonr: 0.0149237\n",
      "[447]\ttraining's pearsonr: 0.0684216\tvalid_1's pearsonr: 0.0149187\n",
      "[448]\ttraining's pearsonr: 0.0684625\tvalid_1's pearsonr: 0.0149252\n",
      "[449]\ttraining's pearsonr: 0.0685104\tvalid_1's pearsonr: 0.0149206\n",
      "[450]\ttraining's pearsonr: 0.06862\tvalid_1's pearsonr: 0.01492\n",
      "[451]\ttraining's pearsonr: 0.0686474\tvalid_1's pearsonr: 0.0149334\n",
      "[452]\ttraining's pearsonr: 0.0686967\tvalid_1's pearsonr: 0.0149422\n",
      "[453]\ttraining's pearsonr: 0.0687533\tvalid_1's pearsonr: 0.0149251\n",
      "[454]\ttraining's pearsonr: 0.0688108\tvalid_1's pearsonr: 0.014916\n",
      "[455]\ttraining's pearsonr: 0.0688595\tvalid_1's pearsonr: 0.0149079\n",
      "[456]\ttraining's pearsonr: 0.0689111\tvalid_1's pearsonr: 0.0149127\n",
      "[457]\ttraining's pearsonr: 0.0689343\tvalid_1's pearsonr: 0.014914\n",
      "[458]\ttraining's pearsonr: 0.0689763\tvalid_1's pearsonr: 0.014911\n",
      "[459]\ttraining's pearsonr: 0.0690131\tvalid_1's pearsonr: 0.0149201\n",
      "[460]\ttraining's pearsonr: 0.0690598\tvalid_1's pearsonr: 0.0149134\n",
      "[461]\ttraining's pearsonr: 0.0691544\tvalid_1's pearsonr: 0.0149193\n",
      "[462]\ttraining's pearsonr: 0.0692178\tvalid_1's pearsonr: 0.0149109\n",
      "[463]\ttraining's pearsonr: 0.0692444\tvalid_1's pearsonr: 0.0149235\n",
      "[464]\ttraining's pearsonr: 0.0692895\tvalid_1's pearsonr: 0.0149319\n",
      "[465]\ttraining's pearsonr: 0.0693603\tvalid_1's pearsonr: 0.0149388\n",
      "[466]\ttraining's pearsonr: 0.0694102\tvalid_1's pearsonr: 0.0149542\n",
      "[467]\ttraining's pearsonr: 0.0694519\tvalid_1's pearsonr: 0.014942\n",
      "[468]\ttraining's pearsonr: 0.0694993\tvalid_1's pearsonr: 0.014938\n",
      "[469]\ttraining's pearsonr: 0.0696065\tvalid_1's pearsonr: 0.0149418\n",
      "[470]\ttraining's pearsonr: 0.0696322\tvalid_1's pearsonr: 0.0149518\n",
      "[471]\ttraining's pearsonr: 0.069676\tvalid_1's pearsonr: 0.01496\n",
      "[472]\ttraining's pearsonr: 0.0697239\tvalid_1's pearsonr: 0.014954\n",
      "[473]\ttraining's pearsonr: 0.0697583\tvalid_1's pearsonr: 0.0149719\n",
      "[474]\ttraining's pearsonr: 0.0697969\tvalid_1's pearsonr: 0.0149792\n",
      "[475]\ttraining's pearsonr: 0.0698478\tvalid_1's pearsonr: 0.0149792\n",
      "[476]\ttraining's pearsonr: 0.0698901\tvalid_1's pearsonr: 0.0149713\n",
      "[477]\ttraining's pearsonr: 0.0699466\tvalid_1's pearsonr: 0.0149637\n",
      "[478]\ttraining's pearsonr: 0.0699946\tvalid_1's pearsonr: 0.0149455\n",
      "[479]\ttraining's pearsonr: 0.0700988\tvalid_1's pearsonr: 0.0149472\n",
      "[480]\ttraining's pearsonr: 0.0701428\tvalid_1's pearsonr: 0.0149534\n",
      "[481]\ttraining's pearsonr: 0.0701801\tvalid_1's pearsonr: 0.0149599\n",
      "[482]\ttraining's pearsonr: 0.0702183\tvalid_1's pearsonr: 0.0149603\n",
      "[483]\ttraining's pearsonr: 0.0702638\tvalid_1's pearsonr: 0.0149476\n",
      "[484]\ttraining's pearsonr: 0.0703127\tvalid_1's pearsonr: 0.0149512\n",
      "[485]\ttraining's pearsonr: 0.0703342\tvalid_1's pearsonr: 0.0149653\n",
      "[486]\ttraining's pearsonr: 0.0704269\tvalid_1's pearsonr: 0.0149654\n",
      "[487]\ttraining's pearsonr: 0.0704731\tvalid_1's pearsonr: 0.014965\n",
      "[488]\ttraining's pearsonr: 0.0705254\tvalid_1's pearsonr: 0.0149566\n",
      "[489]\ttraining's pearsonr: 0.0705877\tvalid_1's pearsonr: 0.0149635\n",
      "[490]\ttraining's pearsonr: 0.0706118\tvalid_1's pearsonr: 0.0149742\n",
      "[491]\ttraining's pearsonr: 0.070644\tvalid_1's pearsonr: 0.0149633\n",
      "[492]\ttraining's pearsonr: 0.0706881\tvalid_1's pearsonr: 0.0149556\n",
      "[493]\ttraining's pearsonr: 0.0707321\tvalid_1's pearsonr: 0.0149663\n",
      "[494]\ttraining's pearsonr: 0.0707908\tvalid_1's pearsonr: 0.0149744\n",
      "[495]\ttraining's pearsonr: 0.0708489\tvalid_1's pearsonr: 0.0149905\n",
      "[496]\ttraining's pearsonr: 0.0709246\tvalid_1's pearsonr: 0.0149932\n",
      "[497]\ttraining's pearsonr: 0.0709637\tvalid_1's pearsonr: 0.0149861\n",
      "[498]\ttraining's pearsonr: 0.071\tvalid_1's pearsonr: 0.0149747\n",
      "[499]\ttraining's pearsonr: 0.0710361\tvalid_1's pearsonr: 0.0149733\n",
      "[500]\ttraining's pearsonr: 0.0710811\tvalid_1's pearsonr: 0.0149762\n",
      "[501]\ttraining's pearsonr: 0.0711234\tvalid_1's pearsonr: 0.0149832\n",
      "[502]\ttraining's pearsonr: 0.0711705\tvalid_1's pearsonr: 0.0149834\n",
      "[503]\ttraining's pearsonr: 0.0712627\tvalid_1's pearsonr: 0.0149896\n",
      "[504]\ttraining's pearsonr: 0.0713025\tvalid_1's pearsonr: 0.0149837\n",
      "[505]\ttraining's pearsonr: 0.0713488\tvalid_1's pearsonr: 0.0149713\n",
      "[506]\ttraining's pearsonr: 0.0714349\tvalid_1's pearsonr: 0.0149808\n",
      "[507]\ttraining's pearsonr: 0.0714784\tvalid_1's pearsonr: 0.0149825\n",
      "[508]\ttraining's pearsonr: 0.0715502\tvalid_1's pearsonr: 0.0149754\n",
      "[509]\ttraining's pearsonr: 0.0715855\tvalid_1's pearsonr: 0.0149654\n",
      "[510]\ttraining's pearsonr: 0.0716194\tvalid_1's pearsonr: 0.014969\n",
      "[511]\ttraining's pearsonr: 0.0717173\tvalid_1's pearsonr: 0.0149695\n",
      "[512]\ttraining's pearsonr: 0.0717568\tvalid_1's pearsonr: 0.0149651\n",
      "[513]\ttraining's pearsonr: 0.0717844\tvalid_1's pearsonr: 0.0149567\n",
      "[514]\ttraining's pearsonr: 0.0718372\tvalid_1's pearsonr: 0.014964\n",
      "[515]\ttraining's pearsonr: 0.071913\tvalid_1's pearsonr: 0.014978\n",
      "[516]\ttraining's pearsonr: 0.071945\tvalid_1's pearsonr: 0.014987\n",
      "[517]\ttraining's pearsonr: 0.0719891\tvalid_1's pearsonr: 0.0149835\n",
      "[518]\ttraining's pearsonr: 0.0720787\tvalid_1's pearsonr: 0.0149779\n",
      "[519]\ttraining's pearsonr: 0.0721521\tvalid_1's pearsonr: 0.0149916\n",
      "[520]\ttraining's pearsonr: 0.0721825\tvalid_1's pearsonr: 0.0149791\n",
      "[521]\ttraining's pearsonr: 0.0722232\tvalid_1's pearsonr: 0.0149785\n",
      "[522]\ttraining's pearsonr: 0.072286\tvalid_1's pearsonr: 0.0149843\n",
      "[523]\ttraining's pearsonr: 0.0723252\tvalid_1's pearsonr: 0.0149834\n",
      "[524]\ttraining's pearsonr: 0.0723873\tvalid_1's pearsonr: 0.0149887\n",
      "[525]\ttraining's pearsonr: 0.0724393\tvalid_1's pearsonr: 0.0149896\n",
      "[526]\ttraining's pearsonr: 0.072474\tvalid_1's pearsonr: 0.0149812\n",
      "[527]\ttraining's pearsonr: 0.0725199\tvalid_1's pearsonr: 0.0149755\n",
      "[528]\ttraining's pearsonr: 0.0726049\tvalid_1's pearsonr: 0.0149725\n",
      "[529]\ttraining's pearsonr: 0.0726548\tvalid_1's pearsonr: 0.0149731\n",
      "[530]\ttraining's pearsonr: 0.0726973\tvalid_1's pearsonr: 0.0149747\n",
      "[531]\ttraining's pearsonr: 0.0727284\tvalid_1's pearsonr: 0.0149626\n",
      "[532]\ttraining's pearsonr: 0.0727654\tvalid_1's pearsonr: 0.0149606\n",
      "[533]\ttraining's pearsonr: 0.072789\tvalid_1's pearsonr: 0.0149559\n",
      "[534]\ttraining's pearsonr: 0.0728368\tvalid_1's pearsonr: 0.0149561\n",
      "[535]\ttraining's pearsonr: 0.0729232\tvalid_1's pearsonr: 0.0149606\n",
      "[536]\ttraining's pearsonr: 0.0729593\tvalid_1's pearsonr: 0.0149651\n",
      "[537]\ttraining's pearsonr: 0.0729985\tvalid_1's pearsonr: 0.0149746\n",
      "[538]\ttraining's pearsonr: 0.0730497\tvalid_1's pearsonr: 0.0149788\n",
      "[539]\ttraining's pearsonr: 0.0731017\tvalid_1's pearsonr: 0.0149785\n",
      "[540]\ttraining's pearsonr: 0.0731607\tvalid_1's pearsonr: 0.0149838\n",
      "[541]\ttraining's pearsonr: 0.0731862\tvalid_1's pearsonr: 0.0149721\n",
      "[542]\ttraining's pearsonr: 0.0732344\tvalid_1's pearsonr: 0.0149666\n",
      "[543]\ttraining's pearsonr: 0.0733036\tvalid_1's pearsonr: 0.0149796\n",
      "[544]\ttraining's pearsonr: 0.0733737\tvalid_1's pearsonr: 0.0149795\n",
      "[545]\ttraining's pearsonr: 0.07341\tvalid_1's pearsonr: 0.0149787\n",
      "[546]\ttraining's pearsonr: 0.0734926\tvalid_1's pearsonr: 0.0149771\n",
      "[547]\ttraining's pearsonr: 0.0735341\tvalid_1's pearsonr: 0.014969\n",
      "[548]\ttraining's pearsonr: 0.073564\tvalid_1's pearsonr: 0.014965\n",
      "[549]\ttraining's pearsonr: 0.0735925\tvalid_1's pearsonr: 0.0149527\n",
      "[550]\ttraining's pearsonr: 0.0736369\tvalid_1's pearsonr: 0.0149476\n",
      "[551]\ttraining's pearsonr: 0.0736845\tvalid_1's pearsonr: 0.0149512\n",
      "[552]\ttraining's pearsonr: 0.0737187\tvalid_1's pearsonr: 0.0149495\n",
      "[553]\ttraining's pearsonr: 0.0738013\tvalid_1's pearsonr: 0.0149465\n",
      "[554]\ttraining's pearsonr: 0.0738341\tvalid_1's pearsonr: 0.0149522\n",
      "[555]\ttraining's pearsonr: 0.0738754\tvalid_1's pearsonr: 0.0149583\n",
      "[556]\ttraining's pearsonr: 0.073943\tvalid_1's pearsonr: 0.0149574\n",
      "[557]\ttraining's pearsonr: 0.073982\tvalid_1's pearsonr: 0.0149498\n",
      "[558]\ttraining's pearsonr: 0.0740416\tvalid_1's pearsonr: 0.0149558\n",
      "[559]\ttraining's pearsonr: 0.0741072\tvalid_1's pearsonr: 0.0149546\n",
      "[560]\ttraining's pearsonr: 0.0741893\tvalid_1's pearsonr: 0.0149586\n",
      "[561]\ttraining's pearsonr: 0.0742488\tvalid_1's pearsonr: 0.0149551\n",
      "[562]\ttraining's pearsonr: 0.0743103\tvalid_1's pearsonr: 0.0149611\n",
      "[563]\ttraining's pearsonr: 0.0743446\tvalid_1's pearsonr: 0.0149625\n",
      "[564]\ttraining's pearsonr: 0.0743733\tvalid_1's pearsonr: 0.0149542\n",
      "[565]\ttraining's pearsonr: 0.0744403\tvalid_1's pearsonr: 0.0149653\n",
      "[566]\ttraining's pearsonr: 0.0744899\tvalid_1's pearsonr: 0.0149638\n",
      "[567]\ttraining's pearsonr: 0.0745086\tvalid_1's pearsonr: 0.0149509\n",
      "[568]\ttraining's pearsonr: 0.0745364\tvalid_1's pearsonr: 0.0149449\n",
      "[569]\ttraining's pearsonr: 0.0745695\tvalid_1's pearsonr: 0.0149483\n",
      "[570]\ttraining's pearsonr: 0.0746405\tvalid_1's pearsonr: 0.0149478\n",
      "[571]\ttraining's pearsonr: 0.0746634\tvalid_1's pearsonr: 0.0149398\n",
      "[572]\ttraining's pearsonr: 0.0747011\tvalid_1's pearsonr: 0.0149421\n",
      "[573]\ttraining's pearsonr: 0.0747601\tvalid_1's pearsonr: 0.0149395\n",
      "[574]\ttraining's pearsonr: 0.0748101\tvalid_1's pearsonr: 0.0149443\n",
      "[575]\ttraining's pearsonr: 0.0748547\tvalid_1's pearsonr: 0.0149531\n",
      "[576]\ttraining's pearsonr: 0.0749118\tvalid_1's pearsonr: 0.0149492\n",
      "[577]\ttraining's pearsonr: 0.07499\tvalid_1's pearsonr: 0.0149504\n",
      "[578]\ttraining's pearsonr: 0.0750454\tvalid_1's pearsonr: 0.0149461\n",
      "[579]\ttraining's pearsonr: 0.0751104\tvalid_1's pearsonr: 0.0149461\n",
      "[580]\ttraining's pearsonr: 0.0751714\tvalid_1's pearsonr: 0.0149447\n",
      "[581]\ttraining's pearsonr: 0.0752187\tvalid_1's pearsonr: 0.0149423\n",
      "[582]\ttraining's pearsonr: 0.0752444\tvalid_1's pearsonr: 0.0149363\n",
      "[583]\ttraining's pearsonr: 0.0753011\tvalid_1's pearsonr: 0.0149302\n",
      "[584]\ttraining's pearsonr: 0.0753339\tvalid_1's pearsonr: 0.014934\n",
      "[585]\ttraining's pearsonr: 0.0753747\tvalid_1's pearsonr: 0.0149414\n",
      "[586]\ttraining's pearsonr: 0.0754051\tvalid_1's pearsonr: 0.0149442\n",
      "[587]\ttraining's pearsonr: 0.0754259\tvalid_1's pearsonr: 0.0149362\n",
      "[588]\ttraining's pearsonr: 0.0754817\tvalid_1's pearsonr: 0.0149322\n",
      "[589]\ttraining's pearsonr: 0.075535\tvalid_1's pearsonr: 0.0149325\n",
      "[590]\ttraining's pearsonr: 0.0756127\tvalid_1's pearsonr: 0.0149312\n",
      "[591]\ttraining's pearsonr: 0.0756874\tvalid_1's pearsonr: 0.0149297\n",
      "[592]\ttraining's pearsonr: 0.0757446\tvalid_1's pearsonr: 0.0149265\n",
      "[593]\ttraining's pearsonr: 0.0758081\tvalid_1's pearsonr: 0.014932\n",
      "[594]\ttraining's pearsonr: 0.0758495\tvalid_1's pearsonr: 0.0149309\n",
      "[595]\ttraining's pearsonr: 0.075873\tvalid_1's pearsonr: 0.0149246\n",
      "[596]\ttraining's pearsonr: 0.0758993\tvalid_1's pearsonr: 0.0149241\n",
      "[597]\ttraining's pearsonr: 0.0759547\tvalid_1's pearsonr: 0.0149206\n",
      "[598]\ttraining's pearsonr: 0.0759918\tvalid_1's pearsonr: 0.0149229\n",
      "[599]\ttraining's pearsonr: 0.0760631\tvalid_1's pearsonr: 0.0149234\n",
      "[600]\ttraining's pearsonr: 0.0761178\tvalid_1's pearsonr: 0.0149302\n",
      "[601]\ttraining's pearsonr: 0.0761389\tvalid_1's pearsonr: 0.0149193\n",
      "[602]\ttraining's pearsonr: 0.0762019\tvalid_1's pearsonr: 0.0149298\n",
      "[603]\ttraining's pearsonr: 0.0762456\tvalid_1's pearsonr: 0.0149321\n",
      "[604]\ttraining's pearsonr: 0.0762685\tvalid_1's pearsonr: 0.0149295\n",
      "[605]\ttraining's pearsonr: 0.0762878\tvalid_1's pearsonr: 0.0149215\n",
      "[606]\ttraining's pearsonr: 0.0763381\tvalid_1's pearsonr: 0.0149168\n",
      "[607]\ttraining's pearsonr: 0.076389\tvalid_1's pearsonr: 0.0149171\n",
      "[608]\ttraining's pearsonr: 0.0764538\tvalid_1's pearsonr: 0.0149246\n",
      "[609]\ttraining's pearsonr: 0.0764961\tvalid_1's pearsonr: 0.0149205\n",
      "[610]\ttraining's pearsonr: 0.0765455\tvalid_1's pearsonr: 0.0149154\n",
      "[611]\ttraining's pearsonr: 0.0765984\tvalid_1's pearsonr: 0.0149205\n",
      "[612]\ttraining's pearsonr: 0.0766702\tvalid_1's pearsonr: 0.0149214\n",
      "[613]\ttraining's pearsonr: 0.0767078\tvalid_1's pearsonr: 0.0149244\n",
      "[614]\ttraining's pearsonr: 0.0767223\tvalid_1's pearsonr: 0.0149158\n",
      "[615]\ttraining's pearsonr: 0.0767722\tvalid_1's pearsonr: 0.0149119\n",
      "[616]\ttraining's pearsonr: 0.0768209\tvalid_1's pearsonr: 0.0149116\n",
      "[617]\ttraining's pearsonr: 0.0768598\tvalid_1's pearsonr: 0.0149136\n",
      "[618]\ttraining's pearsonr: 0.0769004\tvalid_1's pearsonr: 0.014909\n",
      "[619]\ttraining's pearsonr: 0.0769309\tvalid_1's pearsonr: 0.01491\n",
      "[620]\ttraining's pearsonr: 0.0769995\tvalid_1's pearsonr: 0.0149085\n",
      "[621]\ttraining's pearsonr: 0.0770174\tvalid_1's pearsonr: 0.0149003\n",
      "[622]\ttraining's pearsonr: 0.0770397\tvalid_1's pearsonr: 0.0149001\n",
      "[623]\ttraining's pearsonr: 0.0770905\tvalid_1's pearsonr: 0.014896\n",
      "[624]\ttraining's pearsonr: 0.0771495\tvalid_1's pearsonr: 0.0148995\n",
      "[625]\ttraining's pearsonr: 0.0771941\tvalid_1's pearsonr: 0.0148964\n",
      "[626]\ttraining's pearsonr: 0.0772096\tvalid_1's pearsonr: 0.0148887\n",
      "[627]\ttraining's pearsonr: 0.0772556\tvalid_1's pearsonr: 0.014883\n",
      "[628]\ttraining's pearsonr: 0.0773175\tvalid_1's pearsonr: 0.0148904\n",
      "[629]\ttraining's pearsonr: 0.0773743\tvalid_1's pearsonr: 0.0148865\n",
      "[630]\ttraining's pearsonr: 0.0773978\tvalid_1's pearsonr: 0.0148844\n",
      "[631]\ttraining's pearsonr: 0.0774621\tvalid_1's pearsonr: 0.0148841\n",
      "[632]\ttraining's pearsonr: 0.0774815\tvalid_1's pearsonr: 0.0148731\n",
      "[633]\ttraining's pearsonr: 0.0775268\tvalid_1's pearsonr: 0.0148723\n",
      "[634]\ttraining's pearsonr: 0.0775644\tvalid_1's pearsonr: 0.0148672\n",
      "[635]\ttraining's pearsonr: 0.0775956\tvalid_1's pearsonr: 0.0148669\n",
      "[636]\ttraining's pearsonr: 0.077663\tvalid_1's pearsonr: 0.0148633\n",
      "[637]\ttraining's pearsonr: 0.0777224\tvalid_1's pearsonr: 0.0148688\n",
      "[638]\ttraining's pearsonr: 0.0777712\tvalid_1's pearsonr: 0.0148644\n",
      "[639]\ttraining's pearsonr: 0.077797\tvalid_1's pearsonr: 0.014862\n",
      "[640]\ttraining's pearsonr: 0.0778178\tvalid_1's pearsonr: 0.0148603\n",
      "[641]\ttraining's pearsonr: 0.0778631\tvalid_1's pearsonr: 0.0148632\n",
      "[642]\ttraining's pearsonr: 0.0779071\tvalid_1's pearsonr: 0.0148568\n",
      "[643]\ttraining's pearsonr: 0.0779534\tvalid_1's pearsonr: 0.0148486\n",
      "[644]\ttraining's pearsonr: 0.0780169\tvalid_1's pearsonr: 0.0148565\n",
      "[645]\ttraining's pearsonr: 0.0780828\tvalid_1's pearsonr: 0.0148653\n",
      "[646]\ttraining's pearsonr: 0.078104\tvalid_1's pearsonr: 0.0148641\n",
      "[647]\ttraining's pearsonr: 0.0781492\tvalid_1's pearsonr: 0.0148552\n",
      "[648]\ttraining's pearsonr: 0.0782006\tvalid_1's pearsonr: 0.0148605\n",
      "[649]\ttraining's pearsonr: 0.0782307\tvalid_1's pearsonr: 0.014858\n",
      "[650]\ttraining's pearsonr: 0.0783195\tvalid_1's pearsonr: 0.0148612\n",
      "[651]\ttraining's pearsonr: 0.0783831\tvalid_1's pearsonr: 0.0148685\n",
      "[652]\ttraining's pearsonr: 0.0784133\tvalid_1's pearsonr: 0.0148594\n",
      "[653]\ttraining's pearsonr: 0.0784721\tvalid_1's pearsonr: 0.0148688\n",
      "[654]\ttraining's pearsonr: 0.0785588\tvalid_1's pearsonr: 0.0148678\n",
      "[655]\ttraining's pearsonr: 0.0786211\tvalid_1's pearsonr: 0.0148648\n",
      "[656]\ttraining's pearsonr: 0.0786669\tvalid_1's pearsonr: 0.0148601\n",
      "[657]\ttraining's pearsonr: 0.0786955\tvalid_1's pearsonr: 0.0148638\n",
      "[658]\ttraining's pearsonr: 0.0787618\tvalid_1's pearsonr: 0.0148721\n",
      "[659]\ttraining's pearsonr: 0.0788134\tvalid_1's pearsonr: 0.0148651\n",
      "[660]\ttraining's pearsonr: 0.0788555\tvalid_1's pearsonr: 0.0148761\n",
      "[661]\ttraining's pearsonr: 0.0789153\tvalid_1's pearsonr: 0.0148743\n",
      "[662]\ttraining's pearsonr: 0.078943\tvalid_1's pearsonr: 0.0148757\n",
      "[663]\ttraining's pearsonr: 0.0789574\tvalid_1's pearsonr: 0.0148684\n",
      "[664]\ttraining's pearsonr: 0.0790405\tvalid_1's pearsonr: 0.0148651\n",
      "[665]\ttraining's pearsonr: 0.0790638\tvalid_1's pearsonr: 0.0148626\n",
      "[666]\ttraining's pearsonr: 0.0790947\tvalid_1's pearsonr: 0.0148487\n",
      "[667]\ttraining's pearsonr: 0.0791381\tvalid_1's pearsonr: 0.0148406\n",
      "[668]\ttraining's pearsonr: 0.0792013\tvalid_1's pearsonr: 0.0148435\n",
      "[669]\ttraining's pearsonr: 0.079259\tvalid_1's pearsonr: 0.0148461\n",
      "[670]\ttraining's pearsonr: 0.0793411\tvalid_1's pearsonr: 0.0148416\n",
      "[671]\ttraining's pearsonr: 0.0793989\tvalid_1's pearsonr: 0.0148408\n",
      "[672]\ttraining's pearsonr: 0.0794146\tvalid_1's pearsonr: 0.0148385\n",
      "[673]\ttraining's pearsonr: 0.0794642\tvalid_1's pearsonr: 0.0148303\n",
      "[674]\ttraining's pearsonr: 0.0794919\tvalid_1's pearsonr: 0.0148267\n",
      "[675]\ttraining's pearsonr: 0.0795485\tvalid_1's pearsonr: 0.0148335\n",
      "[676]\ttraining's pearsonr: 0.0796272\tvalid_1's pearsonr: 0.0148322\n",
      "[677]\ttraining's pearsonr: 0.0796853\tvalid_1's pearsonr: 0.0148282\n",
      "[678]\ttraining's pearsonr: 0.0797466\tvalid_1's pearsonr: 0.014834\n",
      "[679]\ttraining's pearsonr: 0.0798154\tvalid_1's pearsonr: 0.0148305\n",
      "[680]\ttraining's pearsonr: 0.0798929\tvalid_1's pearsonr: 0.0148235\n",
      "[681]\ttraining's pearsonr: 0.0799386\tvalid_1's pearsonr: 0.0148179\n",
      "[682]\ttraining's pearsonr: 0.0799788\tvalid_1's pearsonr: 0.0148226\n",
      "[683]\ttraining's pearsonr: 0.0800301\tvalid_1's pearsonr: 0.0148282\n",
      "[684]\ttraining's pearsonr: 0.0800908\tvalid_1's pearsonr: 0.0148324\n",
      "[685]\ttraining's pearsonr: 0.0801164\tvalid_1's pearsonr: 0.0148228\n",
      "[686]\ttraining's pearsonr: 0.0801905\tvalid_1's pearsonr: 0.0148144\n",
      "[687]\ttraining's pearsonr: 0.0802238\tvalid_1's pearsonr: 0.0148213\n",
      "[688]\ttraining's pearsonr: 0.0802646\tvalid_1's pearsonr: 0.0148128\n",
      "[689]\ttraining's pearsonr: 0.0802917\tvalid_1's pearsonr: 0.0148137\n",
      "[690]\ttraining's pearsonr: 0.0803195\tvalid_1's pearsonr: 0.0148221\n",
      "[691]\ttraining's pearsonr: 0.0803453\tvalid_1's pearsonr: 0.0148193\n",
      "[692]\ttraining's pearsonr: 0.0804168\tvalid_1's pearsonr: 0.0148083\n",
      "[693]\ttraining's pearsonr: 0.0804367\tvalid_1's pearsonr: 0.0148083\n",
      "[694]\ttraining's pearsonr: 0.0804785\tvalid_1's pearsonr: 0.0148013\n",
      "[695]\ttraining's pearsonr: 0.0805327\tvalid_1's pearsonr: 0.0148021\n",
      "[696]\ttraining's pearsonr: 0.0805798\tvalid_1's pearsonr: 0.014799\n",
      "[697]\ttraining's pearsonr: 0.0806482\tvalid_1's pearsonr: 0.0147899\n",
      "[698]\ttraining's pearsonr: 0.0806729\tvalid_1's pearsonr: 0.0147972\n",
      "[699]\ttraining's pearsonr: 0.080727\tvalid_1's pearsonr: 0.0147931\n",
      "[700]\ttraining's pearsonr: 0.0807478\tvalid_1's pearsonr: 0.0147902\n",
      "[701]\ttraining's pearsonr: 0.0808168\tvalid_1's pearsonr: 0.014781\n",
      "[702]\ttraining's pearsonr: 0.0808814\tvalid_1's pearsonr: 0.014796\n",
      "[703]\ttraining's pearsonr: 0.0809433\tvalid_1's pearsonr: 0.0147985\n",
      "[704]\ttraining's pearsonr: 0.0810008\tvalid_1's pearsonr: 0.0148029\n",
      "[705]\ttraining's pearsonr: 0.0810092\tvalid_1's pearsonr: 0.0147946\n",
      "[706]\ttraining's pearsonr: 0.0810735\tvalid_1's pearsonr: 0.01478\n",
      "[707]\ttraining's pearsonr: 0.0811094\tvalid_1's pearsonr: 0.0147686\n",
      "[708]\ttraining's pearsonr: 0.0811507\tvalid_1's pearsonr: 0.0147766\n",
      "[709]\ttraining's pearsonr: 0.0812016\tvalid_1's pearsonr: 0.0147827\n",
      "[710]\ttraining's pearsonr: 0.0812246\tvalid_1's pearsonr: 0.0147904\n",
      "[711]\ttraining's pearsonr: 0.0812892\tvalid_1's pearsonr: 0.0147792\n",
      "[712]\ttraining's pearsonr: 0.0813401\tvalid_1's pearsonr: 0.0147749\n",
      "[713]\ttraining's pearsonr: 0.0813876\tvalid_1's pearsonr: 0.0147705\n",
      "[714]\ttraining's pearsonr: 0.081435\tvalid_1's pearsonr: 0.0147613\n",
      "[715]\ttraining's pearsonr: 0.0814786\tvalid_1's pearsonr: 0.0147685\n",
      "[716]\ttraining's pearsonr: 0.0815403\tvalid_1's pearsonr: 0.0147708\n",
      "[717]\ttraining's pearsonr: 0.0815663\tvalid_1's pearsonr: 0.0147732\n",
      "[718]\ttraining's pearsonr: 0.0815774\tvalid_1's pearsonr: 0.014766\n",
      "[719]\ttraining's pearsonr: 0.08164\tvalid_1's pearsonr: 0.0147547\n",
      "[720]\ttraining's pearsonr: 0.0816808\tvalid_1's pearsonr: 0.0147486\n",
      "[721]\ttraining's pearsonr: 0.081698\tvalid_1's pearsonr: 0.0147572\n",
      "[722]\ttraining's pearsonr: 0.081744\tvalid_1's pearsonr: 0.0147527\n",
      "[723]\ttraining's pearsonr: 0.0818027\tvalid_1's pearsonr: 0.0147442\n",
      "[724]\ttraining's pearsonr: 0.0818504\tvalid_1's pearsonr: 0.0147379\n",
      "[725]\ttraining's pearsonr: 0.0818991\tvalid_1's pearsonr: 0.0147337\n",
      "[726]\ttraining's pearsonr: 0.0819445\tvalid_1's pearsonr: 0.0147261\n",
      "[727]\ttraining's pearsonr: 0.0819992\tvalid_1's pearsonr: 0.0147302\n",
      "[728]\ttraining's pearsonr: 0.0820565\tvalid_1's pearsonr: 0.0147191\n",
      "[729]\ttraining's pearsonr: 0.0820778\tvalid_1's pearsonr: 0.0147273\n",
      "[730]\ttraining's pearsonr: 0.0820967\tvalid_1's pearsonr: 0.0147148\n",
      "[731]\ttraining's pearsonr: 0.08212\tvalid_1's pearsonr: 0.0147132\n",
      "[732]\ttraining's pearsonr: 0.0821773\tvalid_1's pearsonr: 0.014705\n",
      "[733]\ttraining's pearsonr: 0.0821939\tvalid_1's pearsonr: 0.0147115\n",
      "[734]\ttraining's pearsonr: 0.0822331\tvalid_1's pearsonr: 0.0147052\n",
      "[735]\ttraining's pearsonr: 0.0822882\tvalid_1's pearsonr: 0.0147092\n",
      "[736]\ttraining's pearsonr: 0.0823456\tvalid_1's pearsonr: 0.0147088\n",
      "[737]\ttraining's pearsonr: 0.0823963\tvalid_1's pearsonr: 0.0147041\n",
      "[738]\ttraining's pearsonr: 0.0824378\tvalid_1's pearsonr: 0.0146971\n",
      "[739]\ttraining's pearsonr: 0.0824931\tvalid_1's pearsonr: 0.0146981\n",
      "[740]\ttraining's pearsonr: 0.0825114\tvalid_1's pearsonr: 0.0146954\n",
      "[741]\ttraining's pearsonr: 0.0825724\tvalid_1's pearsonr: 0.0147098\n",
      "[742]\ttraining's pearsonr: 0.0825971\tvalid_1's pearsonr: 0.0147187\n",
      "[743]\ttraining's pearsonr: 0.0826411\tvalid_1's pearsonr: 0.014714\n",
      "[744]\ttraining's pearsonr: 0.0826492\tvalid_1's pearsonr: 0.0147044\n",
      "[745]\ttraining's pearsonr: 0.0826847\tvalid_1's pearsonr: 0.0147026\n",
      "[746]\ttraining's pearsonr: 0.0827224\tvalid_1's pearsonr: 0.0146962\n",
      "[747]\ttraining's pearsonr: 0.0827704\tvalid_1's pearsonr: 0.0146943\n",
      "[748]\ttraining's pearsonr: 0.08282\tvalid_1's pearsonr: 0.0146793\n",
      "[749]\ttraining's pearsonr: 0.0828467\tvalid_1's pearsonr: 0.0146858\n",
      "[750]\ttraining's pearsonr: 0.0828844\tvalid_1's pearsonr: 0.0146815\n",
      "[751]\ttraining's pearsonr: 0.0829057\tvalid_1's pearsonr: 0.0146809\n",
      "[752]\ttraining's pearsonr: 0.0829413\tvalid_1's pearsonr: 0.0146852\n",
      "[753]\ttraining's pearsonr: 0.0829731\tvalid_1's pearsonr: 0.0146745\n",
      "[754]\ttraining's pearsonr: 0.0830108\tvalid_1's pearsonr: 0.0146749\n",
      "[755]\ttraining's pearsonr: 0.083063\tvalid_1's pearsonr: 0.0146665\n",
      "[756]\ttraining's pearsonr: 0.0830818\tvalid_1's pearsonr: 0.0146648\n",
      "[757]\ttraining's pearsonr: 0.0831366\tvalid_1's pearsonr: 0.0146643\n",
      "[758]\ttraining's pearsonr: 0.0831875\tvalid_1's pearsonr: 0.0146683\n",
      "[759]\ttraining's pearsonr: 0.083211\tvalid_1's pearsonr: 0.01467\n",
      "[760]\ttraining's pearsonr: 0.0832576\tvalid_1's pearsonr: 0.014656\n",
      "[761]\ttraining's pearsonr: 0.0833017\tvalid_1's pearsonr: 0.0146497\n",
      "[762]\ttraining's pearsonr: 0.0833443\tvalid_1's pearsonr: 0.0146535\n",
      "[763]\ttraining's pearsonr: 0.0833841\tvalid_1's pearsonr: 0.0146532\n",
      "[764]\ttraining's pearsonr: 0.0834327\tvalid_1's pearsonr: 0.0146377\n",
      "[765]\ttraining's pearsonr: 0.0834731\tvalid_1's pearsonr: 0.0146294\n",
      "[766]\ttraining's pearsonr: 0.0835123\tvalid_1's pearsonr: 0.0146255\n",
      "[767]\ttraining's pearsonr: 0.0835478\tvalid_1's pearsonr: 0.0146178\n",
      "[768]\ttraining's pearsonr: 0.0835914\tvalid_1's pearsonr: 0.0146035\n",
      "[769]\ttraining's pearsonr: 0.083614\tvalid_1's pearsonr: 0.0146111\n",
      "[770]\ttraining's pearsonr: 0.0836387\tvalid_1's pearsonr: 0.0146147\n",
      "[771]\ttraining's pearsonr: 0.0836645\tvalid_1's pearsonr: 0.0146173\n",
      "[772]\ttraining's pearsonr: 0.0837109\tvalid_1's pearsonr: 0.0146037\n",
      "[773]\ttraining's pearsonr: 0.0837531\tvalid_1's pearsonr: 0.0145975\n",
      "[774]\ttraining's pearsonr: 0.0837592\tvalid_1's pearsonr: 0.0146024\n",
      "[775]\ttraining's pearsonr: 0.0838001\tvalid_1's pearsonr: 0.0145973\n",
      "[776]\ttraining's pearsonr: 0.0838421\tvalid_1's pearsonr: 0.0146055\n",
      "[777]\ttraining's pearsonr: 0.0838889\tvalid_1's pearsonr: 0.0146101\n",
      "[778]\ttraining's pearsonr: 0.0839344\tvalid_1's pearsonr: 0.0145911\n",
      "[779]\ttraining's pearsonr: 0.0839719\tvalid_1's pearsonr: 0.0145871\n",
      "[780]\ttraining's pearsonr: 0.0839949\tvalid_1's pearsonr: 0.0145722\n",
      "[781]\ttraining's pearsonr: 0.0840349\tvalid_1's pearsonr: 0.0145688\n",
      "[782]\ttraining's pearsonr: 0.0840644\tvalid_1's pearsonr: 0.0145655\n",
      "[783]\ttraining's pearsonr: 0.0841075\tvalid_1's pearsonr: 0.0145517\n",
      "[784]\ttraining's pearsonr: 0.0841413\tvalid_1's pearsonr: 0.0145547\n",
      "[785]\ttraining's pearsonr: 0.0841895\tvalid_1's pearsonr: 0.0145585\n",
      "[786]\ttraining's pearsonr: 0.0842268\tvalid_1's pearsonr: 0.0145613\n",
      "[787]\ttraining's pearsonr: 0.0842704\tvalid_1's pearsonr: 0.0145452\n",
      "[788]\ttraining's pearsonr: 0.0843156\tvalid_1's pearsonr: 0.0145461\n",
      "[789]\ttraining's pearsonr: 0.0843249\tvalid_1's pearsonr: 0.0145384\n",
      "[790]\ttraining's pearsonr: 0.0843843\tvalid_1's pearsonr: 0.0145435\n",
      "[791]\ttraining's pearsonr: 0.0844296\tvalid_1's pearsonr: 0.0145547\n",
      "[792]\ttraining's pearsonr: 0.0844369\tvalid_1's pearsonr: 0.0145466\n",
      "[793]\ttraining's pearsonr: 0.0844757\tvalid_1's pearsonr: 0.0145267\n",
      "[794]\ttraining's pearsonr: 0.0845331\tvalid_1's pearsonr: 0.0145404\n",
      "[795]\ttraining's pearsonr: 0.0845757\tvalid_1's pearsonr: 0.0145354\n",
      "[796]\ttraining's pearsonr: 0.0845999\tvalid_1's pearsonr: 0.014533\n",
      "Early stopping, best iteration is:\n",
      "[496]\ttraining's pearsonr: 0.0709246\tvalid_1's pearsonr: 0.0149932\n",
      "elapsed_time:531.0289669036865[sec]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "# Training just with Securities with hight target_spread and validated with Securities with low target_spread.\n",
    "\n",
    "# features =['High','Low','Open','Close','Volume', 'return_1month', 'return_2month', 'return_3month', 'volatility_1month', 'volatility_2month', 'volatility_3month',\n",
    "#        'MA_gap_1month', 'MA_gap_2month', 'MA_gap_3month']\n",
    "features =['High','Low','Open','Close','Volume',\n",
    "       'MA_gap_1month', 'MA_gap_2month', 'MA_gap_3month']\n",
    "# features =['High','Low','Open','Close','Volume',]\n",
    "train=fill_nan_inf(train)\n",
    "val=fill_nan_inf(val)\n",
    "all_data = fill_nan_inf(all_data)\n",
    "params_lgb = {'learning_rate': 0.005,'metric':'None','objective': 'regression','boosting': 'gbdt','verbosity': 0,'n_jobs': -1,'force_col_wise':True}  \n",
    "\n",
    "train_dataset = lgb.Dataset(all_data[all_data['SecuritiesCode'].isin(list_spred_h)][features],all_data[all_data['SecuritiesCode'].isin(list_spred_h)][\"Target\"],feature_name = features )\n",
    "val_dataset = lgb.Dataset(all_data[all_data['SecuritiesCode'].isin(list_spred_l)][features], all_data[all_data['SecuritiesCode'].isin(list_spred_l)][\"Target\"],feature_name = features)\n",
    "\n",
    "model = lgb.train(params = params_lgb, \n",
    "                train_set = train_dataset, \n",
    "                valid_sets = [train_dataset, val_dataset], \n",
    "                num_boost_round = 3000, \n",
    "                feval=feval_pearsonr,\n",
    "                callbacks=[ lgb.early_stopping(stopping_rounds=300, verbose=True)])    \n",
    "\n",
    "elapsed_time = time.time() - start\n",
    "print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prescription: SAA policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000353936029523914\n"
     ]
    }
   ],
   "source": [
    "#make prediction with our lightgbm model on the test set\n",
    "preds = model.predict(test[features])\n",
    "\n",
    "#compute the mse of the set on the test test\n",
    "mse = mean_squared_error(test[\"Target\"], preds)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nDecisionTreeRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Julien Pinède\\Documents\\MIT\\Courses\\Machine Learning Under Modern Optimization Lenses\\project\\code\\test_lightgbm.ipynb Cellule 16\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Julien%20Pin%C3%A8de/Documents/MIT/Courses/Machine%20Learning%20Under%20Modern%20Optimization%20Lenses/project/code/test_lightgbm.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m tree\u001b[39m.\u001b[39mDecisionTreeRegressor(criterion\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msquared_error\u001b[39m\u001b[39m\"\u001b[39m, min_samples_leaf\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Julien%20Pin%C3%A8de/Documents/MIT/Courses/Machine%20Learning%20Under%20Modern%20Optimization%20Lenses/project/code/test_lightgbm.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(all_data[features], all_data[\u001b[39m\"\u001b[39m\u001b[39mTarget\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Julien%20Pin%C3%A8de/Documents/MIT/Courses/Machine%20Learning%20Under%20Modern%20Optimization%20Lenses/project/code/test_lightgbm.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m preds_cart \u001b[39m=\u001b[39m  model\u001b[39m.\u001b[39;49mpredict(test[features])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Julien%20Pin%C3%A8de/Documents/MIT/Courses/Machine%20Learning%20Under%20Modern%20Optimization%20Lenses/project/code/test_lightgbm.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m mse \u001b[39m=\u001b[39m mean_squared_error(test[\u001b[39m\"\u001b[39m\u001b[39mTarget\u001b[39m\u001b[39m\"\u001b[39m], preds_cart)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Julien%20Pin%C3%A8de/Documents/MIT/Courses/Machine%20Learning%20Under%20Modern%20Optimization%20Lenses/project/code/test_lightgbm.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(mse)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\OR_env\\lib\\site-packages\\sklearn\\tree\\_classes.py:505\u001b[0m, in \u001b[0;36mBaseDecisionTree.predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39m\"\"\"Predict class or regression value for X.\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \n\u001b[0;32m    484\u001b[0m \u001b[39mFor a classification model, the predicted class for each sample in X is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    502\u001b[0m \u001b[39m    The predicted classes, or the predict values.\u001b[39;00m\n\u001b[0;32m    503\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    504\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> 505\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_X_predict(X, check_input)\n\u001b[0;32m    506\u001b[0m proba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtree_\u001b[39m.\u001b[39mpredict(X)\n\u001b[0;32m    507\u001b[0m n_samples \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\OR_env\\lib\\site-packages\\sklearn\\tree\\_classes.py:471\u001b[0m, in \u001b[0;36mBaseDecisionTree._validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[39m\"\"\"Validate the training data on predict (probabilities).\"\"\"\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[39mif\u001b[39;00m check_input:\n\u001b[1;32m--> 471\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, dtype\u001b[39m=\u001b[39;49mDTYPE, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    472\u001b[0m     \u001b[39mif\u001b[39;00m issparse(X) \u001b[39mand\u001b[39;00m (\n\u001b[0;32m    473\u001b[0m         X\u001b[39m.\u001b[39mindices\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mintc \u001b[39mor\u001b[39;00m X\u001b[39m.\u001b[39mindptr\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mintc\n\u001b[0;32m    474\u001b[0m     ):\n\u001b[0;32m    475\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\OR_env\\lib\\site-packages\\sklearn\\base.py:577\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    575\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    576\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 577\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[0;32m    578\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    579\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\OR_env\\lib\\site-packages\\sklearn\\utils\\validation.py:899\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    893\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    894\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    895\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    896\u001b[0m         )\n\u001b[0;32m    898\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 899\u001b[0m         _assert_all_finite(\n\u001b[0;32m    900\u001b[0m             array,\n\u001b[0;32m    901\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[0;32m    902\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[0;32m    903\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    904\u001b[0m         )\n\u001b[0;32m    906\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    907\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\OR_env\\lib\\site-packages\\sklearn\\utils\\validation.py:146\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    125\u001b[0m             \u001b[39mnot\u001b[39;00m allow_nan\n\u001b[0;32m    126\u001b[0m             \u001b[39mand\u001b[39;00m estimator_name\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    131\u001b[0m             \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    132\u001b[0m             msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m    133\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    145\u001b[0m             )\n\u001b[1;32m--> 146\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n\u001b[0;32m    148\u001b[0m \u001b[39m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39melif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mdtype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan:\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nDecisionTreeRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "model = tree.DecisionTreeRegressor(criterion=\"squared_error\", min_samples_leaf=2)\n",
    "model = model.fit(all_data[features], all_data[\"Target\"])\n",
    "\n",
    "preds_cart =  model.predict(test[features])\n",
    "mse = mean_squared_error(test[\"Target\"], preds_cart)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather in the same sets all the datapoints of the train set that fall in the same leaf of the tree as the datapoints of the test set\n",
    "\n",
    "for i, row in test.iterrows():\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('OR_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1976cb1e7b5e6cb0fb6c6121e2125643fa054d3ec707007008fc4887e664439"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
